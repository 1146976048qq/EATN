{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUWG3ehrbuub"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kr_KJ_kI17i7"
   },
   "source": [
    "# install required env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "colab_type": "code",
    "id": "cHfpkQpZb2QL",
    "outputId": "25ab024c-cb0c-41b6-af5c-e9d7483443e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
      "Collecting ftfy==4.4.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (0.1.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (1.12.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "Successfully built ftfy\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-4.4.3\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 8.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "Successfully built regex\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2019.6.8\n"
     ]
    }
   ],
   "source": [
    "#! pip install pytorch-pretrained-bert\n",
    "! pip install spacy ftfy==4.4.3\n",
    "! python -m spacy download en\n",
    "! pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZ4ErZIGde3g"
   },
   "outputs": [],
   "source": [
    "#! pip install torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "colab_type": "code",
    "id": "hpfVal7k4Mqa",
    "outputId": "83fedbd7-8414-491e-e415-ce2cc63b59b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 10.0MB/s \n",
      "\u001b[?25hCollecting livelossplot\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/f6/0618c30078f9c1e4b2cd84f1ea6bb70c6615070468b75b0d934326107bcd/livelossplot-0.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot) (3.0.3)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot) (5.2.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.5.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (1.1.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (2.10.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.3.2)\n",
      "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.5.3)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.2.4)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.8.2)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.5.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.6.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot) (1.1.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot) (4.4.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot) (17.0.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (1.4.2)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.3)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.8.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (3.1.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (2.1.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot) (5.5.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot) (2.6.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot) (0.5.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.7.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.8.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (1.0.16)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (4.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.1.7)\n",
      "Installing collected packages: tensorboardX, livelossplot\n",
      "Successfully installed livelossplot-0.4.1 tensorboardX-1.7\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm tensorboardX livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "2InLwo0HcQhN",
    "outputId": "24211dea-9a0a-4c49-b661-14db0668a6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HNPJHYwWcZWF",
    "outputId": "34c17a96-b918-4720-84b9-8b08d34fa2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvlL45sQlal1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwlkSIV5d1So"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UV36W5L7drIt"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('./gdrive/My Drive/deepqa_features/')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOl3QowrZcg4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from deepqa_models import TextNormalization, MultiLabel_Vectorizer, DocProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azRnJLMwi1-K"
   },
   "source": [
    "## Text Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sro2v3RmdrLD"
   },
   "outputs": [],
   "source": [
    "text_clf_file = './gdrive/My Drive/deepqa_features/data/cuishou_text_intent.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfA22RXCdrNz"
   },
   "outputs": [],
   "source": [
    "text_clf_df = pd.read_csv(text_clf_file, encoding='gbk', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "05pvvPsZdrQM",
    "outputId": "dcecf4a0-4f17-4243-994e-02f658c74ba0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>表明来意</td>\n",
       "      <td>噢您好，我们这边是金葫芦公司的，您在这边有一个欠款，啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有意愿还款</td>\n",
       "      <td>啊就想知道那个我手机出了问题，那个你们那个APP本都不见了，然后我找不到这块，下面加个微信，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>表明来意</td>\n",
       "      <td>我们这边是那个嗯金湖路公司的，您在这边有一笔欠款怎么还没处理事忘了吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>表明来意</td>\n",
       "      <td>金葫芦公司的，您在这边有一笔欠款怎么还没处理丫，你是忘了吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>核资</td>\n",
       "      <td>能还上吗！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0   表明来意                        噢您好，我们这边是金葫芦公司的，您在这边有一个欠款，啊\n",
       "1  有意愿还款  啊就想知道那个我手机出了问题，那个你们那个APP本都不见了，然后我找不到这块，下面加个微信，...\n",
       "2   表明来意                我们这边是那个嗯金湖路公司的，您在这边有一笔欠款怎么还没处理事忘了吗？\n",
       "3   表明来意                     金葫芦公司的，您在这边有一笔欠款怎么还没处理丫，你是忘了吗？\n",
       "4     核资                                              能还上吗！"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz2MgdYLdrSs"
   },
   "outputs": [],
   "source": [
    "tn = TextNormalization(stop_words='./gdrive/My Drive/deepqa_features/data/chinese_punct.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "iPbA4pGNZLva",
    "outputId": "7d0d329c-76fd-4f0e-9392-5c9646846135"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level  2\n",
      "Process multiple sentences ===> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.985 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.985 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3885\n"
     ]
    }
   ],
   "source": [
    "clean_text = tn.fit_transform(text_clf_df['text'])\n",
    "print(len(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChI6UJn1ZLyl"
   },
   "outputs": [],
   "source": [
    "y_mvec = MultiLabel_Vectorizer(onehot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3MTclreNZLsO",
    "outputId": "3a67a83d-afc4-444e-8eb0-4d16fdaaf557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3885,)\n"
     ]
    }
   ],
   "source": [
    "y_mvec.fit(text_clf_df['label'])\n",
    "y_label = y_mvec.transform(text_clf_df['label'])\n",
    "print(y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "BRz9usmhZLml",
    "outputId": "c01c5c18-4e0d-4f6e-d0d7-297741bc360c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level  2\n",
      "Process multiple sentences ===> \n",
      "level  2\n",
      "Process multiple sentences ===> \n"
     ]
    }
   ],
   "source": [
    "doc_proc = DocProcessor()\n",
    "doc_proc.fit(text_clf_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FkH0MJXHdrVU"
   },
   "outputs": [],
   "source": [
    "doc_lens, seq_lens = doc_proc.get_docs_sents_length_no_pad(text_clf_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kiJBrvOB73mY",
    "outputId": "a37867d6-4662-4614-f00f-776190541e04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.0"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(doc_lens, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "eMb5BSMT73qC",
    "outputId": "b66bf1bd-66f5-44e0-8320-43fcdc6e000f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 12 ['询问关系', '有意愿还款', '第三人还款', '礼貌挂断', '标准开场', '施压警告', '核资', '表明来意', '无意愿还款', '客户抱怨', '无还款能力', '有助于还款'] \n",
      " {'询问关系': 0, '有意愿还款': 1, '第三人还款': 2, '礼貌挂断': 3, '标准开场': 4, '施压警告': 5, '核资': 6, '表明来意': 7, '无意愿还款': 8, '客户抱怨': 9, '无还款能力': 10, '有助于还款': 11}\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 80\n",
    "n_labels = y_mvec.n_labels\n",
    "label_list = y_mvec.unique_labels\n",
    "label_map = y_mvec.label2int_dict\n",
    "\n",
    "print(max_seq_len, n_labels, label_list, '\\n'  , label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7m2TpnTi8ooO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDdXK6-Xh6KZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkmKjJUAh6OI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qf3gibnuqPrB"
   },
   "source": [
    "## NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ckOb4I_qOgO"
   },
   "outputs": [],
   "source": [
    "text_ner_file = './gdrive/My Drive/deepqa_features/data/ner_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U105_nioqVMC"
   },
   "outputs": [],
   "source": [
    "text_ner_df = pd.read_csv(text_ner_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "oqN_wrxkqVSp",
    "outputId": "d10b114a-4c36-4f65-83dd-c9cf175002ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。</td>\n",
       "      <td>O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 ...</td>\n",
       "      <td>B-LOC B-LOC O O O O O O O O O O O O O O B-LOC ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence                                                tag\n",
       "0                海 钓 比 赛 地 点 在 厦 门 与 金 门 之 间 的 海 域 。  O O O O O O O B-LOC I-LOC O B-LOC I-LOC O O O ...\n",
       "1  这 座 依 山 傍 水 的 博 物 馆 由 国 内 一 流 的 设 计 师 主 持 设 计 ...  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
       "2  但 作 为 一 个 共 产 党 员 、 人 民 公 仆 ， 应 当 胸 怀 宽 阔 ， 真 ...  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
       "3  在 发 达 国 家 ， 急 救 保 险 十 分 普 及 ， 已 成 为 社 会 保 障 体 ...  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
       "4  日 俄 两 国 国 内 政 局 都 充 满 变 数 ， 尽 管 日 俄 关 系 目 前 是 ...  B-LOC B-LOC O O O O O O O O O O O O O O B-LOC ..."
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "cHiOGWyPqVPj",
    "outputId": "860f399f-7db5-40ef-fbcc-5ea2357f5c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level  2\n",
      "Process multiple sentences ===> \n",
      "level  2\n",
      "Process multiple sentences ===> \n"
     ]
    }
   ],
   "source": [
    "y_ner_proc = DocProcessor(use_end=True, use_start=False, use_unk=False, use_pad=True) \n",
    "y_ner_proc.fit(text_ner_df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p0RREhZ0q41I",
    "outputId": "1297dd40-e6dd-47d9-8dd4-7c1eb136506b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 577 {'<pad>': 0, '<eos>': 1, 'O': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-PER': 5, 'I-PER': 6, 'B-ORG': 7, 'I-ORG': 8}\n"
     ]
    }
   ],
   "source": [
    "n_labels = y_ner_proc.vocab_size\n",
    "print(n_labels, y_ner_proc.max_seq_len, y_ner_proc.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "IgMxuFFJq48U",
    "outputId": "d5084ff9-ea38-40b6-e37a-8206a29a97af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<eos>': 1,\n",
       " '<pad>': 0,\n",
       " 'B-LOC': 3,\n",
       " 'B-ORG': 7,\n",
       " 'B-PER': 5,\n",
       " 'I-LOC': 4,\n",
       " 'I-ORG': 8,\n",
       " 'I-PER': 6,\n",
       " 'O': 2}"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ner_proc.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rYxDlWhq5BE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqqfCQsWq5Fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlBL2KS9qUf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mNy3j1gqUk5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9rsaYIaqUci"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kf1dHSW7qUZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUjFLNfGqWWq"
   },
   "source": [
    "## Phrase Sim Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ypw7HWrAq5K8"
   },
   "outputs": [],
   "source": [
    "text_phrase_file = './gdrive/My Drive/deepqa_features/data/cuishou_phrase_inference.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLS15ZBGqacO"
   },
   "outputs": [],
   "source": [
    "text_phrase_df = pd.read_csv(text_phrase_file,encoding='gbk', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "mdmwEmk-qaZs",
    "outputId": "9cb42335-0a26-4bb8-b97a-47805aba8268"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>唉 我 这边 缴费 的 麻烦 你 必须 要 打个 电话 给 你 处理 一下 描述 的 欠款 ...</td>\n",
       "      <td>嗯 嗯 再见</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>你 不能 不能 我 尽量 快点</td>\n",
       "      <td>您好 我 这边 是 我 想 借 公司 的 你 在 我 想 借 的 欠款 已经 逾期 了 现在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>唉 你好 我 想 借 贷款 这边 逾期 怎么 一直 没 处理 啊</td>\n",
       "      <td>嗯 六点 之前 可以 啊 只要 您 这 段时间 你 能 确保 把 这个 钱 还 进来 可以 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>今天 还接 不了 你 跟 我 说 的 是 点到 点 之间 肯定 发工资 是 吧 肯定 会 结账 的</td>\n",
       "      <td>你好 我 这边 是 这个 金葫芦 的 你 在 金 湖路 一笔 欠款 今天 是 还款 日 您 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>嗯 其他 没有 走 没有 转</td>\n",
       "      <td>是 什么 的 因为 这 两天 小孩 在 住院 不行 你 说</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  ...                                         sentence_B\n",
       "0      0  ...                                             嗯 嗯 再见\n",
       "1      0  ...  您好 我 这边 是 我 想 借 公司 的 你 在 我 想 借 的 欠款 已经 逾期 了 现在...\n",
       "2      0  ...  嗯 六点 之前 可以 啊 只要 您 这 段时间 你 能 确保 把 这个 钱 还 进来 可以 ...\n",
       "3      0  ...  你好 我 这边 是 这个 金葫芦 的 你 在 金 湖路 一笔 欠款 今天 是 还款 日 您 ...\n",
       "4      1  ...                      是 什么 的 因为 这 两天 小孩 在 住院 不行 你 说\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_phrase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5NFmsNnqaWI"
   },
   "outputs": [],
   "source": [
    "p_proc = DocProcessor()\n",
    "\n",
    "doc_a, sent_a = p_proc.get_docs_sents_length_no_pad(text_phrase_df['sentence_A'])\n",
    "doc_b, sent_a = p_proc.get_docs_sents_length_no_pad(text_phrase_df['sentence_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ynaZmQvZqaRY",
    "outputId": "d093a876-42bd-4736-877f-b5556e93aaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 315 \n",
      " 120.0 109.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(doc_a), np.max(doc_b) ,'\\n', np.percentile(doc_a, 95), np.percentile(doc_b, 95) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VmsnxIVqaOB"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbll8JRDqaKz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hE_36MkWhSl0"
   },
   "source": [
    "## QA UCAC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIPJ2YJmq45C"
   },
   "outputs": [],
   "source": [
    "ucac_train_file = './gdrive/My Drive/deepqa_features/data/ucac_qa/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPJ4DHhkhuXc"
   },
   "outputs": [],
   "source": [
    "#ucac_data = pd.read_json(ucac_train_file, lines=True)\n",
    "ucac_train_df = pd.read_csv(ucac_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "deE8iq5-huUa",
    "outputId": "2c4a407f-098c-4177-e4a6-9a00c20955e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_selected</th>\n",
       "      <th>passage</th>\n",
       "      <th>passage_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>键能和解离能有什么区别 由键能求焓变公式：δh=反应物的总键能-生成物的总键能</td>\n",
       "      <td>9</td>\n",
       "      <td>键能和解离能有什么区别</td>\n",
       "      <td>10133193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hold住是什么意思？网络用语hold住的起源 整个场面我要hold住 表演结束后，miss...</td>\n",
       "      <td>4</td>\n",
       "      <td>我hold得住了是什么意思</td>\n",
       "      <td>10072299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>如何理解法的古老含义和现代特征 （三）现代社会法的特征 现代社会的法，虽然也有刑法，对犯罪分...</td>\n",
       "      <td>20</td>\n",
       "      <td>如何理解法的特征</td>\n",
       "      <td>10052345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>上海-东京-大溪地，签证和行李问题请教打什么～  问题3.日本的签证停留期是15天，若回上海...</td>\n",
       "      <td>16</td>\n",
       "      <td>去大溪地行李直挂问题</td>\n",
       "      <td>10037814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>水浒传：二龙山兴衰史 不久后，武松因为醉打蒋门神被陷害，几乎被设计杀死。最终凭借高超的武艺，...</td>\n",
       "      <td>7</td>\n",
       "      <td>水浒传里的二龙山</td>\n",
       "      <td>10089946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_selected  ... question_id\n",
       "0            0  ...    10133193\n",
       "1            0  ...    10072299\n",
       "2            0  ...    10052345\n",
       "3            0  ...    10037814\n",
       "4            0  ...    10089946\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucac_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PGeiBWr_huRi",
    "outputId": "9c8cf0d8-b9a2-4acd-cafb-7eae4204b8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucac_train_df['is_selected'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "9x4OjnQGhuNQ",
    "outputId": "70f23a56-6570-42ed-8111-b3f48962fcec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.8627\n",
       "1    0.1373\n",
       "Name: is_selected, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucac_train_df['is_selected'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQlbFDdRkZc_"
   },
   "outputs": [],
   "source": [
    "p_proc = DocProcessor()\n",
    "\n",
    "doc_a, sent_a = p_proc.get_docs_sents_length_no_pad(ucac_train_df['passage'])\n",
    "doc_b, sent_a = p_proc.get_docs_sents_length_no_pad(ucac_train_df['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uRzXg2AwkZjB",
    "outputId": "41f8fd60-d206-48ae-9c10-a27ab632577a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104510 87 388.0 18.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(doc_a), np.max(doc_b), np.percentile(doc_a,95), np.percentile(doc_b, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mer3RttwkZpl",
    "outputId": "94786975-7f16-41b3-ee72-a5f0b9bdf3f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n"
     ]
    }
   ],
   "source": [
    "doc_seq_len = 388\n",
    "query_seq_len = 18\n",
    "max_seq_len = doc_seq_len + query_seq_len\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guY-NaUekZvz"
   },
   "outputs": [],
   "source": [
    "label_map = {0:0, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyahjFqakZ21"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkEue2ZokZ_X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "at36XBjpP6BV"
   },
   "source": [
    "## Aspect sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S1pJYknP4hc"
   },
   "outputs": [],
   "source": [
    "#aspect_prefix = './gdrive/My Drive/deepqa_features/data/data_after_process/aspect_term/2014_laptop/'\n",
    "aspect_prefix = '../../data/Experiment/data_after_process/aspect_term/2014_laptop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anMrib2AP45L"
   },
   "outputs": [],
   "source": [
    "aspect_laptop_train = aspect_prefix + 'train.csv'\n",
    "aspect_laptop_test = aspect_prefix + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WqQT0dJP42Q"
   },
   "outputs": [],
   "source": [
    "laptop_train_df = pd.read_csv(aspect_laptop_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "RbFtGQeFP4zb",
    "outputId": "2e810c8c-82b3-4863-8c56-00d9c0d9102f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentence</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I charge it at night and skip taking the $T$ w...</td>\n",
       "      <td>cord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>battery life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The tech guy then said the $T$ does not do 1-t...</td>\n",
       "      <td>service center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>`` sales '' team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The $T$ then said the service center does not ...</td>\n",
       "      <td>tech guy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                           sentence  \\\n",
       "0         0  I charge it at night and skip taking the $T$ w...   \n",
       "1         1  I charge it at night and skip taking the cord ...   \n",
       "2        -1  The tech guy then said the $T$ does not do 1-t...   \n",
       "3        -1  The tech guy then said the service center does...   \n",
       "4         0  The $T$ then said the service center does not ...   \n",
       "\n",
       "               term  \n",
       "0              cord  \n",
       "1      battery life  \n",
       "2    service center  \n",
       "3  `` sales '' team  \n",
       "4          tech guy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "TjL1LEm1P4wk",
    "outputId": "ebb3a966-a9cd-4a41-aa8e-f491e15052bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentence</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i think the $T$ is very nice but the operating...</td>\n",
       "      <td>computer screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>i think the computer screen is very nice but t...</td>\n",
       "      <td>operating system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I am pleased with $T$ , but the windows 8 oper...</td>\n",
       "      <td>the life of battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>I am pleased with the life of battery , but th...</td>\n",
       "      <td>windows 8 operating system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>$T$ is super fast , around anywhere from 35 s...</td>\n",
       "      <td>Boot time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                           sentence  \\\n",
       "0         1  i think the $T$ is very nice but the operating...   \n",
       "1        -1  i think the computer screen is very nice but t...   \n",
       "2         1  I am pleased with $T$ , but the windows 8 oper...   \n",
       "3        -1  I am pleased with the life of battery , but th...   \n",
       "4         1   $T$ is super fast , around anywhere from 35 s...   \n",
       "\n",
       "                         term  \n",
       "0             computer screen  \n",
       "1            operating system  \n",
       "2         the life of battery  \n",
       "3  windows 8 operating system  \n",
       "4                   Boot time  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_test_df = pd.read_csv(aspect_laptop_test)\n",
    "laptop_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VzhIj6lP4tb"
   },
   "outputs": [],
   "source": [
    "def fill_token(sent, term):\n",
    "  #print(sent, term)\n",
    "  return sent.replace('$T$', term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfHUc4QhP4qr"
   },
   "outputs": [],
   "source": [
    "laptop_train_df['sent'] = laptop_train_df.apply(lambda x: fill_token(x[1], x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VBnFCyqP4ny"
   },
   "outputs": [],
   "source": [
    "p_proc = DocProcessor()\n",
    "\n",
    "doc_a, sent_a = p_proc.get_docs_sents_length_no_pad(laptop_train_df['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BkjMPOWMP4eO",
    "outputId": "cb23a7c1-8264-4b3c-c19a-08a2d6a7ca20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 185.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(doc_a), np.percentile(doc_a, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdqXhBU7TPdF"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "GQX9X0phP4Zr",
    "outputId": "cd5318ff-505c-4e28-8e0d-a6c77128d2a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    994\n",
       "-1    870\n",
       " 0    464\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_train_df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHBkJ4FoUB2L"
   },
   "outputs": [],
   "source": [
    "label_map = {1:2, -1:0, 0:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITRa5Xn4UB8K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ig9UU1RUCBs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obVAbeD8d3o6"
   },
   "source": [
    "# Bert Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "colab_type": "code",
    "id": "zgEf8gZXdrXp",
    "outputId": "921c3eb1-67a9-4765-a2c4-1b1d099f78f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. mask = tensor<(6,), int64, cuda:0>\n",
      "Starting var:.. x = tensor<(3,), float32, cuda:0>\n",
      "12:52:23.904396 call         8 def myfunc(mask, x):\n",
      "12:52:23.904551 line         9     y = torch.zeros(6).to(device)\n",
      "New var:....... y = tensor<(6,), float32, cuda:0>\n",
      "12:52:23.904767 line        10     y.masked_scatter_(mask, x)\n",
      "12:52:23.908930 exception   10     y.masked_scatter_(mask, x)\n",
      "RuntimeError: Expected object of scalar type Byte but got scalar type Long for argument #2 'mask'\n",
      "Call ended by exception\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-abf3283811f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pysnooper/tracer.py\u001b[0m in \u001b[0;36msimple_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msimple_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-abf3283811f2>\u001b[0m in \u001b[0;36mmyfunc\u001b[0;34m(mask, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_scatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Byte but got scalar type Long for argument #2 'mask'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchsnooper\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "\n",
    "@torchsnooper.snoop()\n",
    "def myfunc(mask, x):\n",
    "    y = torch.zeros(6).to(device)\n",
    "    y.masked_scatter_(mask, x)\n",
    "    return y\n",
    "\n",
    "mask = torch.tensor([0, 1, 0, 1, 1, 0], device='cuda')\n",
    "source = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "y = myfunc(mask, source).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "Sg2KM1ordraF",
    "outputId": "59e796e8-9dea-4c12-cac3-4a2d4a3e9373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpnk4b1_yq\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 2331844.27B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpnk4b1_yq to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpnk4b1_yq\n",
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "etzdbjG5drcu",
    "outputId": "32c86daf-d1b1-4455-d6c5-c32d4231e78b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmp_cj3fame\n",
      "100%|██████████| 407873900/407873900 [00:06<00:00, 65159992.58B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmp_cj3fame to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmp_cj3fame\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp2n50cl43\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "3kvLMdMxdrfl",
    "outputId": "25fb5c2b-8e46-4339-cd5f-722cc36b0f7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmphyllpbmk\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2rfklvVdriD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LreXuRDBnKDO",
    "outputId": "209008eb-f87f-421f-9d1d-979f11982a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['噢', '您', '好', '我', '们', '这', '边', '是', '金', '葫', '芦', '公', '司', '的', '您', '在', '这', '边', '有', '一', '个', '欠', '款', '啊']\n"
     ]
    }
   ],
   "source": [
    "text = text_clf_df.loc[0, 'text']\n",
    "text_e1 = clean_text[0]\n",
    "tokenized_text = tokenizer.tokenize(text_e1)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LfSfNL5LnKHE",
    "outputId": "0b66a561-b2b1-4799-95c1-9c9b63ba13d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1688, 2644, 1962, 2769, 812, 6821, 6804, 3221, 7032, 5872, 5701, 1062, 1385, 4638, 2644, 1762, 6821, 6804, 3300, 671, 702, 3612, 3621, 1557]\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Q-jRtMuYnKJv",
    "outputId": "f6650ccc-3041-4c02-f205-50dbc852d4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "噢您好，我们这边是金葫芦公司的，您在这边有一个欠款，啊 \n",
      " 噢 您好 我们 这边 是 金葫芦 公司 的 您 在 这边 有 一个 欠款 啊\n"
     ]
    }
   ],
   "source": [
    "print(text, '\\n',text_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "heE_Xs06nKMY",
    "outputId": "d391e399-237d-458b-a25d-e55476a3d0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['噢', '您', '好', '，', '我', '们', '这', '边', '是', '金', '葫', '芦', '公', '司', '的', '，', '您', '在', '这', '边', '有', '一', '个', '欠', '款', '，', '啊']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcsx7KVllpIA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ne9KusVlpLo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fdBVzPqnMVz"
   },
   "source": [
    "# Bert Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V5dUjTvNbRMd",
    "outputId": "25706a14-aa1e-43db-b5d6-4f0a8bae6367"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./gdrive/My Drive/pytorch-pretrained-BERT/')\n",
    "\n",
    "from pytorch_pretrained_bert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTmOPh9jnKRR"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "        \n",
    "def convert_examples_to_features(examples, label_map, max_seq_length,\n",
    "                                 tokenizer, output_mode):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      examples: Inputexamples from dataprocessor\n",
    "      label_map: dict, {label: label_index}\n",
    "      max_seq_length: int, max_length the sentence after padding\n",
    "      tokenizer: BertTokenizer\n",
    "      output_mode: str, 'classification' or 'regression'\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    #label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            print(example.label)\n",
    "            #label_id = label_map[example.label]\n",
    "            label_id = label_map.get(example.label, -1)\n",
    "        elif output_mode == \"regression\":\n",
    "            label_id = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logging.info(\"*** Example ***\")\n",
    "            logging.info(\"guid: %s\" % (example.guid))\n",
    "            logging.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logging.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dx5laIDGnP5t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines        \n",
    "        \n",
    "\n",
    "class TextClfProcessor(DataProcessor):\n",
    "    \"\"\"\n",
    "    This is the text processor for text classification task dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_columns='text', label_columns='label'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          #data_dir: prefix of path of data files, \n",
    "          feature_columns: str or list, column names of features\n",
    "          label_columns: str or list, column names of labels\n",
    "        \"\"\"\n",
    "        self.feature_columns = feature_columns\n",
    "        self.label_columns = label_columns\n",
    "        self.labels = None\n",
    "    \n",
    "    def load_datafile(self, filename=None, df=None, data_type='train', size=-1, labels_available=True):\n",
    "        assert (filename is not None or df is not None), ('Must specify filename or df to read data')\n",
    "        \n",
    "        if filename is not None:\n",
    "          logging.info('Loading {} dataset : {}'.format(data_type, filename))\n",
    "          df = pd.read_csv(filename)\n",
    "        if size == -1:            \n",
    "            return self._create_examples(df, data_type, labels_available)\n",
    "        else:\n",
    "            return self._create_examples(df.sample(size), data_type, labels_available)\n",
    "    \n",
    "    def get_train_examples(self, filename=None, df=None, size=-1, labels_available=True):\n",
    "        return self.load_datafile(filename, df, 'train', size, labels_available)\n",
    "        \n",
    "    def get_dev_examples(self, filename=None, df=None, size=-1, labels_available=True):\n",
    "        return self.load_datafile(filename, df, 'dev', size, labels_available)\n",
    "    \n",
    "    def get_test_examples(self, filename=None, df=None, size=-1, labels_available=False):\n",
    "        return self.load_datafile(filename, df, 'test', size, labels_available)\n",
    "\n",
    "    def get_labels(self):\n",
    "        if self.labels == None:\n",
    "            self.labels = list(pd.read_csv(os.path.join(self.data_dir, \"classes.txt\"),header=None)[0].values)\n",
    "        return self.labels\n",
    "      \n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        examples = []\n",
    "        for index, row in df.iterrows():\n",
    "          text_a = row.get(self.feature_columns, '')\n",
    "          label = ''\n",
    "          if labels_available:\n",
    "            label = row.get(self.label_columns, '')\n",
    "          \n",
    "          examples.append(InputExample(guid=index, text_a=text_a, label=label))\n",
    "        return examples\n",
    "          \n",
    "\n",
    "class TextNERProcessor(TextClfProcessor):\n",
    "    \"\"\"\n",
    "    This is the NER classification text processor.\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, feature_columns='text', label_columns='tag'):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        feature_columns: str or list, column names of features\n",
    "        label_columns: str or list, column names of labels\n",
    "        labels: list of all ner labels\n",
    "      \"\"\"\n",
    "      super(TextNERProcessor, self).__init__(feature_columns, label_columns)\n",
    "      self.labels = labels\n",
    "      \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "      \n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        examples = []\n",
    "        for index, row in df.iterrows():\n",
    "            text_a = row.get(self.feature_columns, '')\n",
    "            label = ''\n",
    "            if labels_available:\n",
    "                label = row.get(self.label_columns, '')\n",
    "            \n",
    "            examples.append(InputExample(guid=index, text_a=text_a, label=label))\n",
    "        return examples\n",
    "      \n",
    "      \n",
    "class TextPhraseSimProcessor(TextClfProcessor):\n",
    "    \"\"\"\n",
    "    This is the phrase sim comparision text processor\n",
    "    \"\"\"\n",
    "    def __init__(self, labels=None, feature_columns=['sentence_A', 'sentence_B'], label_columns='label'):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        feature_columns: str or list, column names of features\n",
    "        label_columns: str or list, column names of labels\n",
    "        labels: list of all ner labels\n",
    "      \"\"\"\n",
    "      super(TextPhraseSimProcessor, self).__init__(feature_columns, label_columns)\n",
    "      self.labels = labels\n",
    "      \n",
    "    def get_labels(self):\n",
    "      return self.labels\n",
    "    \n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        examples = []\n",
    "        for index, row in df.iterrows():\n",
    "            text_a = row.get(self.feature_columns[0], '')\n",
    "            text_b = row.get(self.feature_columns[1], '')\n",
    "            label = ''\n",
    "            if labels_available:\n",
    "                label = row.get(self.label_columns, '')\n",
    "            examples.append(InputExample(guid=index, text_a=text_a, text_b=text_b, label=label))              \n",
    "        return examples\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eACNKJeAAO8A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "class BertDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        tensors = tuple(self.convert_datatype(data_) for data_ in tensors)\n",
    "        super(BertDataset, self).__init__(*tensors)\n",
    "    \n",
    "    def check_datatype(self, data_tensor):\n",
    "        return type(data_tensor)==type(torch.tensor([1,2]))\n",
    "        \n",
    "    def convert_datatype(self, data_tensor):\n",
    "        \"\"\"\n",
    "        Convert data_tensor to tensor.LongTensor()\n",
    "        \"\"\"\n",
    "        if not self.check_datatype(data_tensor):\n",
    "            return torch.LongTensor(data_tensor)\n",
    "        else:\n",
    "            return data_tensor\n",
    "   \n",
    "    def shape(self):\n",
    "        return self.data_tensor[0].shape    \n",
    "    \n",
    "    \n",
    "def get_batch(data_examples, label_map, max_seq_length, tokenizer, output_mode=\"classification\",\n",
    "              label_available=True, batch_size=32, num_workers=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_examples: examples from DataProcessor get_*_examples\n",
    "        #label_list: list of all labels\n",
    "        label_map: dict, {label:label_index}\n",
    "        max_seq_length: int, fixed length that sentences are converted to\n",
    "        tokenizer: BertTokenizer\n",
    "        output_mode: task mode, whether it is classification or regression\n",
    "        label_availabel: True, whether there is label in dataset\n",
    "        batch_size: int\n",
    "        num_workers: int, for distributed training\n",
    "    return:\n",
    "        DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "    data = convert_examples_to_features(data_examples, label_map, max_seq_length, tokenizer, output_mode)\n",
    "    \n",
    "    # loop over data\n",
    "    # to do: think an efficient way to process features\n",
    "    input_ids = [f.input_ids for f in data]\n",
    "    input_mask = [f.input_mask for f in data]\n",
    "    segment_ids = [f.segment_ids for f in data]\n",
    "    \n",
    "    if label_available:\n",
    "      label_id = [f.label_id for f in data]\n",
    "      # for train and dev dataset\n",
    "      data_set = BertDataset(input_ids, input_mask, segment_ids, label_id)\n",
    "      \n",
    "      # use sampler\n",
    "      if num_workers == -1:\n",
    "        data_sampler = RandomSampler(data_set)\n",
    "      else:\n",
    "        data_sampler = DistributedSampler(data_set)\n",
    "    \n",
    "    else:\n",
    "      # for test dataset\n",
    "      data_set = BertDataset(input_ids, input_mask, segment_ids)\n",
    "      data_sampler = SequentialSampler(data_set)\n",
    "    \n",
    "    return DataLoader(data_set, sampler=data_sampler, batch_size=batch_size)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22Bc_bSVHfKX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_clf_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6486443957fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClfProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_clf_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_available\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdev_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dev_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_clf_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_clf_df' is not defined"
     ]
    }
   ],
   "source": [
    "X_proc = TextClfProcessor(feature_columns='text', label_columns='label')\n",
    "\n",
    "train_examples = X_proc.get_train_examples(df=text_clf_df, size=-1, labels_available=True)\n",
    "dev_examples = X_proc.get_dev_examples(df=text_clf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "Kc2zf9bEnh9S",
    "outputId": "197cb993-33e4-4afd-bdb1-bccb880a9900"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139450
    },
    "colab_type": "code",
    "id": "vb0v1eh9HfO8",
    "outputId": "58854ae6-cc5c-4fbd-9d0b-e532c0334e3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 3885\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 0\n",
      "INFO:root:tokens: [CLS] 噢 您 好 ， 我 们 这 边 是 金 葫 芦 公 司 的 ， 您 在 这 边 有 一 个 欠 款 ， 啊 [SEP]\n",
      "INFO:root:input_ids: 101 1688 2644 1962 8024 2769 812 6821 6804 3221 7032 5872 5701 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 702 3612 3621 8024 1557 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1\n",
      "INFO:root:tokens: [CLS] 啊 就 想 知 道 那 个 我 手 机 出 了 问 题 ， 那 个 你 们 那 个 app 本 都 不 见 了 ， 然 后 我 找 不 到 这 块 ， 下 面 加 个 微 信 ， 发 个 连 接 给 我 ！ [SEP]\n",
      "INFO:root:input_ids: 101 1557 2218 2682 4761 6887 6929 702 2769 2797 3322 1139 749 7309 7579 8024 6929 702 872 812 6929 702 8172 3315 6963 679 6224 749 8024 4197 1400 2769 2823 679 1168 6821 1779 8024 678 7481 1217 702 2544 928 8024 1355 702 6825 2970 5314 2769 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 有意愿还款 (id = 3)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 2\n",
      "INFO:root:tokens: [CLS] 我 们 这 边 是 那 个 嗯 金 湖 路 公 司 的 ， 您 在 这 边 有 一 笔 欠 款 怎 么 还 没 处 理 事 忘 了 吗 ？ [SEP]\n",
      "INFO:root:input_ids: 101 2769 812 6821 6804 3221 6929 702 1638 7032 3959 6662 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 5011 3612 3621 2582 720 6820 3766 1905 4415 752 2563 749 1408 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 3\n",
      "INFO:root:tokens: [CLS] 金 葫 芦 公 司 的 ， 您 在 这 边 有 一 笔 欠 款 怎 么 还 没 处 理 丫 ， 你 是 忘 了 吗 ？ [SEP]\n",
      "INFO:root:input_ids: 101 7032 5872 5701 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 5011 3612 3621 2582 720 6820 3766 1905 4415 703 8024 872 3221 2563 749 1408 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 4\n",
      "INFO:root:tokens: [CLS] 能 还 上 吗 ！ [SEP]\n",
      "INFO:root:input_ids: 101 5543 6820 677 1408 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 核资 (id = 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有助于还款\n",
      "施压警告\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "有助于还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "客户抱怨\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "无意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "有助于还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "无意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "无意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "无意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "第三人还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "施压警告\n",
      "客户抱怨\n",
      "无意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "无意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "客户抱怨\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "第三人还款\n",
      "第三人还款\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "客户抱怨\n",
      "无还款能力\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "无意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "第三人还款\n",
      "第三人还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "标准开场\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "客户抱怨\n",
      "施压警告\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "核资\n",
      "核资\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "标准开场\n",
      "询问关系\n",
      "施压警告\n",
      "标准开场\n",
      "表明来意\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "标准开场\n",
      "表明来意\n",
      "表明来意\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "标准开场\n",
      "询问关系\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "询问关系\n",
      "核资\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "询问关系\n",
      "询问关系\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "施压警告\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 3885\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 0\n",
      "INFO:root:tokens: [CLS] 噢 您 好 ， 我 们 这 边 是 金 葫 芦 公 司 的 ， 您 在 这 边 有 一 个 欠 款 ， 啊 [SEP]\n",
      "INFO:root:input_ids: 101 1688 2644 1962 8024 2769 812 6821 6804 3221 7032 5872 5701 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 702 3612 3621 8024 1557 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1\n",
      "INFO:root:tokens: [CLS] 啊 就 想 知 道 那 个 我 手 机 出 了 问 题 ， 那 个 你 们 那 个 app 本 都 不 见 了 ， 然 后 我 找 不 到 这 块 ， 下 面 加 个 微 信 ， 发 个 连 接 给 我 ！ [SEP]\n",
      "INFO:root:input_ids: 101 1557 2218 2682 4761 6887 6929 702 2769 2797 3322 1139 749 7309 7579 8024 6929 702 872 812 6929 702 8172 3315 6963 679 6224 749 8024 4197 1400 2769 2823 679 1168 6821 1779 8024 678 7481 1217 702 2544 928 8024 1355 702 6825 2970 5314 2769 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 有意愿还款 (id = 3)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 2\n",
      "INFO:root:tokens: [CLS] 我 们 这 边 是 那 个 嗯 金 湖 路 公 司 的 ， 您 在 这 边 有 一 笔 欠 款 怎 么 还 没 处 理 事 忘 了 吗 ？ [SEP]\n",
      "INFO:root:input_ids: 101 2769 812 6821 6804 3221 6929 702 1638 7032 3959 6662 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 5011 3612 3621 2582 720 6820 3766 1905 4415 752 2563 749 1408 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 3\n",
      "INFO:root:tokens: [CLS] 金 葫 芦 公 司 的 ， 您 在 这 边 有 一 笔 欠 款 怎 么 还 没 处 理 丫 ， 你 是 忘 了 吗 ？ [SEP]\n",
      "INFO:root:input_ids: 101 7032 5872 5701 1062 1385 4638 8024 2644 1762 6821 6804 3300 671 5011 3612 3621 2582 720 6820 3766 1905 4415 703 8024 872 3221 2563 749 1408 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 表明来意 (id = 9)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 4\n",
      "INFO:root:tokens: [CLS] 能 还 上 吗 ！ [SEP]\n",
      "INFO:root:input_ids: 101 5543 6820 677 1408 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 核资 (id = 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "客户抱怨\n",
      "施压警告\n",
      "客户抱怨\n",
      "无还款能力\n",
      "客户抱怨\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有助于还款\n",
      "施压警告\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "有助于还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "客户抱怨\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "无意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "有助于还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "无意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "无意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "无意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "第三人还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "施压警告\n",
      "客户抱怨\n",
      "无意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "客户抱怨\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "无意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "客户抱怨\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有助于还款\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "第三人还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "第三人还款\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "第三人还款\n",
      "第三人还款\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "客户抱怨\n",
      "无还款能力\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无意愿还款\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "无意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "第三人还款\n",
      "第三人还款\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "无还款能力\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "标准开场\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "第三人还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有助于还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有助于还款\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "第三人还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "施压警告\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "有意愿还款\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "无还款能力\n",
      "施压警告\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "无还款能力\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "有助于还款\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "无还款能力\n",
      "有意愿还款\n",
      "无还款能力\n",
      "表明来意\n",
      "核资\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "无还款能力\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "有助于还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "客户抱怨\n",
      "施压警告\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "表明来意\n",
      "无还款能力\n",
      "无还款能力\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "客户抱怨\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "无还款能力\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "礼貌挂断\n",
      "核资\n",
      "核资\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "询问关系\n",
      "施压警告\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "标准开场\n",
      "询问关系\n",
      "施压警告\n",
      "标准开场\n",
      "表明来意\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "标准开场\n",
      "表明来意\n",
      "表明来意\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "标准开场\n",
      "询问关系\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "施压警告\n",
      "询问关系\n",
      "表明来意\n",
      "礼貌挂断\n",
      "表明来意\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "礼貌挂断\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "标准开场\n",
      "表明来意\n",
      "核资\n",
      "礼貌挂断\n",
      "标准开场\n",
      "表明来意\n",
      "标准开场\n",
      "表明来意\n",
      "施压警告\n",
      "施压警告\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "标准开场\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "客户抱怨\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有助于还款\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "有意愿还款\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "客户抱怨\n",
      "客户抱怨\n",
      "表明来意\n",
      "无还款能力\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "有意愿还款\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "无还款能力\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "核资\n",
      "有意愿还款\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "核资\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "询问关系\n",
      "核资\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "询问关系\n",
      "询问关系\n",
      "核资\n",
      "客户抱怨\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "核资\n",
      "核资\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "表明来意\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "表明来意\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "核资\n",
      "施压警告\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "表明来意\n",
      "询问关系\n",
      "核资\n",
      "施压警告\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "施压警告\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "询问关系\n",
      "施压警告\n",
      "有意愿还款\n",
      "核资\n",
      "表明来意\n",
      "核资\n",
      "核资\n",
      "施压警告\n",
      "客户抱怨\n",
      "核资\n",
      "有意愿还款\n",
      "表明来意\n",
      "核资\n",
      "表明来意\n",
      "无还款能力\n",
      "客户抱怨\n",
      "施压警告\n",
      "客户抱怨\n",
      "无还款能力\n",
      "客户抱怨\n"
     ]
    }
   ],
   "source": [
    "train_data = get_batch(train_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)\n",
    "val_data = get_batch(dev_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqhlCF29qKvF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W8VndtxmdQx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkOruFjxnRD6"
   },
   "source": [
    "# Bert Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcxDXG4mnP74"
   },
   "outputs": [],
   "source": [
    "class RunningAverage():\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    batch_size = len(y_true)\n",
    "    y_pred_labels = torch.max(y_pred, 1)[1].view(y_true.size())\n",
    "    num_corr = (y_pred_labels.data==y_true.data).float().sum()\n",
    "    return 100.0*num_corr/batch_size\n",
    "\n",
    "def ner_accuracy(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    #print(type(outputs), type(labels))\n",
    "    #print(len(outputs), labels.shape)\n",
    "    pred_labels = [i for ii in y_pred for i in ii]\n",
    "    true_labels = [i.item() for ii in y_true for i in ii]\n",
    "    #print(len(pred_labels), len(true_labels))\n",
    "    #print(pred_labels[0], true_labels[0])\n",
    "    return accuracy_score(true_labels, pred_labels)*100.0       \n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict of floats in json file\n",
    "    Args:\n",
    "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    import json\n",
    "    with open(json_path, 'w') as f:\n",
    "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "        \n",
    "def save_checkpoint(state, is_best, checkpoint):\n",
    "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
    "    checkpoint + 'best.pth.tar'\n",
    "    Args:\n",
    "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
    "        is_best: (bool) True if it is the best model seen till now\n",
    "        checkpoint: (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import torch\n",
    "    \n",
    "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
    "        os.mkdir(checkpoint)\n",
    "    else:\n",
    "        print(\"Checkpoint Directory exists! \")\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import torch\n",
    "    \n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    # change map_location to cuda as wish\n",
    "    checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "\n",
    "    return checkpoint \n",
    "\n",
    "def load_seq2seq_checkpoint(checkpoint, enc_model, dec_model, enc_optimizer=None, dec_optimizer=None):\n",
    "    \"\"\"\n",
    "    !!! Warnning: This func only works for seq2seq model. DO NOT USE THIS FOR ANYTHING ELSE\n",
    "    \n",
    "    Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        enc_model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        enc_optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "        dec_model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        dec_optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import torch\n",
    "    \n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    # change map_location to cuda as wish\n",
    "    checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "    \n",
    "    enc_model.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    dec_model.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "    if enc_optimizer:\n",
    "        enc_optimizer.load_state_dict(checkpoint['encoder_optim_dict'])\n",
    "    if dec_optimizer:\n",
    "        dec_optimizer.load_state_dict(checkpoint['decoder_optim_dict'])\n",
    "\n",
    "    return checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g76ysp63nP-R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, device, batch_size=32, n_epochs=5, min_clip_val=-1.0, max_clip_val=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            device: cuda or cpu\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.min_clip_val = min_clip_val\n",
    "        self.max_clip_val = max_clip_val\n",
    "        self.device = device\n",
    "    \n",
    "    def clip_gradient(self, model):\n",
    "        params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "        for p in params:\n",
    "            p.grad.data.clamp_(self.min_clip_val, self.max_clip_val)\n",
    "    \n",
    "    def train(self, model, train_data, optimizer,sum_writer, epoch, metrics=None, loss_fn=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_data: dataLoader\n",
    "            model: model instance\n",
    "            optimizer:\n",
    "            sum_writer: tensorboarX SummaryWriter\n",
    "            loss_fn: loss function to do backpropagation\n",
    "            metrics: accuracy, etc\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        from tqdm import trange, tqdm\n",
    "\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "    \n",
    "        # summary for current training loop and a running objct for loss\n",
    "        summ = []\n",
    "        loss_avg = RunningAverage()\n",
    "        #for i, (X_batch, y_batch) in tqdm(enumerate(train_data)):\n",
    "        #for i, (X_batch, y_batch) in enumerate(train_data):\n",
    "        for i, batch in enumerate(tqdm(train_data, desc='train_data_iter')):\n",
    "            # X_batch is a tuple\n",
    "            batch = tuple(x.to(self.device) for x in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            \n",
    "            # clear previous gradients, compute gradients of all variabels wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute model output and loss\n",
    "            loss = model(input_ids, input_mask, segment_ids, label_ids)\n",
    "            pred_batch = model.get_prediction_result()\n",
    "            #print('train loss shape: ', loss.shape)\n",
    "            \n",
    "            if metrics is not None and 'accuracy' in metrics.keys():\n",
    "                acc = metrics['accuracy'](label_ids, pred_batch)\n",
    "            else:\n",
    "                acc = accuracy(label_ids, pred_batch)\n",
    "            #print(y_batch.data.cpu().numpy().shape, pred_batch.data.cpu().numpy().shape)\n",
    "            \n",
    "            # performs updates using calculated gradients\n",
    "            loss.backward()\n",
    "            self.clip_gradient(model)\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % 100==0:            \n",
    "                print('Step:{}, Training Loss: {:05.3f}, Training Accuracy: {:05.2f}'.format(i, loss.item(), acc.item()))\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {}\n",
    "                if metrics is not None:\n",
    "                    summary_batch = {metric: metrics[metric](label_ids, pred_batch).item() for metric in metrics}\n",
    "                summary_batch['loss'] = loss.item()   #loss.data[0]\n",
    "                summ.append(summary_batch)\n",
    "                # add scalars to tensorboardX\n",
    "                for key, val in summary_batch.items():\n",
    "                    sum_writer.add_scalar('train/{}'.format(key), val, epoch)\n",
    "                      \n",
    "                #y_pred_labels = torch.max(pred_batch, 1)[1].view(y_batch.size())\n",
    "                #sum_writer.add_pr_curve('train/pre_recall', y_batch.data.cpu().numpy(), y_pred_labels.data.cpu().numpy(), epoch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.item())\n",
    "            #t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "        # compute mean of all metrics in summary\n",
    "        metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "        metrics_string = '; '.join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "        logging.info('- Train metrics: '+ metrics_string)\n",
    "        return metrics_mean\n",
    "    \n",
    "    def evaluate(self, model, dev_data, optimizer,sum_writer, epoch, metrics=None, loss_fn=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dev_data: dataLoader for dev set\n",
    "            model: model instance\n",
    "            optimizer:\n",
    "            sum_writer: tensorboarX SummaryWriter\n",
    "            loss_fn: loss function to do backpropagation\n",
    "            metrics: accuracy, etc\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        from tqdm import trange, tqdm\n",
    "        import torch\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "    \n",
    "        # summary for current training loop and a running objct for loss\n",
    "        summ = []\n",
    "        loss_avg = RunningAverage()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dev_data):\n",
    "                batch = tuple(x.to(self.device) for x in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                \n",
    "                # compute model output and loss\n",
    "                loss = model(input_ids, input_mask, segment_ids, label_ids)\n",
    "                pred_batch = model.get_logits()\n",
    "                \n",
    "                #print('evaluate loss shape: ', loss.shape)\n",
    "                if metrics is not None and 'accuracy' in metrics.keys():\n",
    "                    acc = metrics['accuracy'](label_ids, pred_batch)\n",
    "                else:\n",
    "                    acc = accuracy(label_ids, pred_batch)\n",
    "                \n",
    "                # Evaluate summaries only once in a while\n",
    "                if i % 100==0:          \n",
    "                    print('Step:{}, Evaluate Loss: {:05.3f}, Evaluate Accuracy: {:05.2f}'.format(i, loss.item(),acc.item()))\n",
    "        \n",
    "                    # compute all metrics on this batch\n",
    "                    summary_batch = {}\n",
    "                    if metrics is not None:\n",
    "                        summary_batch = {metric: metrics[metric](label_ids, pred_batch).item() for metric in metrics}\n",
    "                    summary_batch['loss'] = loss.item()   #loss.data[0]\n",
    "                    summ.append(summary_batch)\n",
    "                    # add scalars to tensorboardX\n",
    "                    for key, val in summary_batch.items():\n",
    "                        sum_writer.add_scalar('evaluate/{}'.format(key), val, epoch)\n",
    "                        \n",
    "                    #y_pred_labels = torch.max(pred_batch, 1)[1].view(y_batch.size())\n",
    "                    #sum_writer.add_pr_curve('evaluate/pre_recall', y_batch.data.cpu().numpy(), y_pred_labels.data.cpu().numpy(), epoch)\n",
    "                    \n",
    "                    #sum_writer.add_pr_curve('evaluate/pre_recall', y_batch.data.cpu().numpy(), pred_batch.data.cpu().numpy(), epoch)\n",
    "                    \n",
    "                # update the average loss\n",
    "                loss_avg.update(loss.item())\n",
    "                #t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "        # compute mean of all metrics in summary\n",
    "        metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "        metrics_string = '; '.join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "        logging.info('- Eval metrics: '+ metrics_string)\n",
    "        return metrics_mean    \n",
    "        \n",
    "    def train_and_evaluate(self, model, train_data, dev_data, optimizer,metrics=None, loss_fn=None,model_dir='./'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_data: dataLoader for train set\n",
    "            dev_data: dataloader for dev set\n",
    "            model: model instance\n",
    "            optimizer:\n",
    "            loss_fn: loss function to do backpropagation\n",
    "            metrics: accuracy, etc\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import logging\n",
    "        from tqdm import trange, tqdm\n",
    "        from tensorboardX import SummaryWriter\n",
    "        from livelossplot import PlotLosses\n",
    "    \n",
    "        best_val_acc = 0.0\n",
    "        sum_writer = SummaryWriter() # add summary of training to tensorboardX\n",
    "        liveloss = PlotLosses()\n",
    "        for epoch in trange(self.n_epochs):\n",
    "            # Run one epoch\n",
    "            logging.info(\"Epoch {}/{}\".format(epoch + 1, self.n_epochs))\n",
    "            logs = {}\n",
    "            # train the model\n",
    "            train_metrics = self.train(model, train_data, optimizer, sum_writer, epoch, metrics, loss_fn)            \n",
    "            val_metrics = self.evaluate(model, dev_data, optimizer, sum_writer, epoch, metrics, loss_fn)\n",
    "        \n",
    "            val_acc = val_metrics['accuracy']\n",
    "            is_best = val_acc >= best_val_acc\n",
    "\n",
    "            # Save weights\n",
    "            save_checkpoint({'epoch': epoch + 1,'state_dict': model.state_dict(),'optim_dict' : optimizer.state_dict()}, \n",
    "                        is_best=is_best, checkpoint=model_dir)\n",
    "            \n",
    "            # If best_eval, best_save_path        \n",
    "            if is_best:\n",
    "                logging.info(\"- Found new best accuracy\")\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "                # Save best val metrics in a json file in the model directory\n",
    "                best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "                save_dict_to_json(val_metrics, best_json_path)\n",
    "            \n",
    "            # draw liveloss plot\n",
    "            for key, val in train_metrics.items():\n",
    "                logs[key] = val\n",
    "            for key, val in val_metrics.items():\n",
    "                logs['val_'+key] = val\n",
    "            liveloss.update(logs)\n",
    "            liveloss.draw()\n",
    "            \n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        save_dict_to_json(val_metrics, last_json_path)\n",
    "        \n",
    "        # export training details to json\n",
    "        #batch_size = train_data.batch_size\n",
    "        #seq_len = train_data.dataset.shape()[1]\n",
    "        #dum_input = torch.zeros(batch_size, seq_len, dtype=torch.int64).to(model.device)\n",
    "        #sum_writer.add_graph(model, dum_input, verbose=True)\n",
    "        sum_writer.export_scalars_to_json(os.path.join(model_dir,'train_scalars.json'))\n",
    "        sum_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUeMGZu3nQAk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J44df7kwnUIN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rV3Mu1dOnU-r"
   },
   "source": [
    "# Bert Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz9K8KqQnULK"
   },
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    \"\"\"\n",
    "    It supports predicting label(int), label(str) and label_proba from a model\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model, max_seq_length, tokenizer, X_proc=None,\n",
    "                 target_int2label_dict=None, target_label2int_dict=None, batch_size=16, num_workers=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          device: cuda or cpu\n",
    "          model: saved or pretrained model\n",
    "          max_seq_length: int\n",
    "          X_proc: DataProcessor\n",
    "          tokenizer: BertTokenizer\n",
    "          target_int2label_dict: dict, {label: label_index}\n",
    "          target_label2int_dict: dict, {label_indx: label}\n",
    "          batch_size: 16\n",
    "          num_workers: -1\n",
    "        \"\"\"\n",
    "        super(Predictor,self).__init__()\n",
    "        self.device = device        \n",
    "        self.model = model.to(device)\n",
    "        self.model.device = device\n",
    "        self.X_proc = X_proc\n",
    "        self.target_int2label_dict = target_int2label_dict\n",
    "        #self.label_list = target_int2label_dict.keys() \n",
    "        self.label_map = target_label2int_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers        \n",
    "        \n",
    "    def preprocess_sentences(self, filename, df):\n",
    "      if self.X_proc is None:\n",
    "        self.X_proc = TextClfProcessor(filename)\n",
    "      test_examples = self.X_proc.get_test_examples(filename, df, size=-1, labels_available=False)\n",
    "      \n",
    "      # Hold input data for returning it \n",
    "      #input_data = [{ 'id': input_example.guid, 'comment_text': input_example.text_a } for input_example in test_examples]\n",
    "      \n",
    "      test_data = get_batch(test_examples, self.label_map, self.max_seq_length, self.tokenizer, output_mode=\"classification\", \n",
    "                            label_available=False, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "      return test_data\n",
    "    \n",
    "    def predict_proba(self, filename=None, df=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          filename: str, path to test_data\n",
    "        \n",
    "        returns: \n",
    "          probability: predicted probability of each label \n",
    "        \"\"\"\n",
    "        import logging\n",
    "        from tqdm import trange, tqdm\n",
    "        \n",
    "        test_data = self.preprocess_sentences(filename, df)\n",
    "               \n",
    "        # set eval mode for the model\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(test_data, desc='predict_data_iter')):\n",
    "                batch = tuple(x.to(self.device) for x in batch)\n",
    "                input_ids, input_mask, segment_ids = batch\n",
    "                \n",
    "                logits = self.model(input_ids, input_mask, segment_ids)\n",
    "                probability = self.model.get_prediction_result() # (batch, n_labels) after activation\n",
    "                logging.info('logits shape {}, probability shape {}'.format(logits.shape, probability.shape))\n",
    "                \n",
    "            return probability\n",
    "            \n",
    "    def predict(self, filename=None, df=None, single_label=True, thres=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename: str, path to test_data\n",
    "            single_label: bool, whether it is a single or multiple label prediction\n",
    "            thres: float, manually set threshold to select a predicted label\n",
    "        returns:\n",
    "            pred_label: actual labels for each data\n",
    "            pred_label_ids: label index for each data\n",
    "            probability: float probability of each label \n",
    "        \"\"\"\n",
    "        # probability gives probability for all labels classes\n",
    "        probability = self.predict_proba(filename, df)\n",
    "        pred_label, pred_probability = self.get_prediction_labels(probability, single_label, thres, use_target_int2label_dict=True)\n",
    "        return pred_label, pred_probability, probability\n",
    "        \n",
    "    def get_label_from_dict(self, pred_pos, use_target_int2label_dict=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_pos: int, index with highest probability\n",
    "            use_target_int2label_dict: bool\n",
    "        \"\"\"\n",
    "        pred_label = pred_pos\n",
    "        if not use_target_int2label_dict:\n",
    "            return pred_label\n",
    "        \n",
    "        if self.target_int2label_dict:\n",
    "            try:\n",
    "                pred_label = self.target_int2label_dict[pred_pos]\n",
    "            except:\n",
    "                pred_label = self.target_int2label_dict[str(pred_pos)]\n",
    "        return pred_label\n",
    "    \n",
    "    def get_prediction_labels(self, pred_proba, single_label=True, thres=None, use_target_int2label_dict=False):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            pred_proba: pred from model\n",
    "            target_int2word_dict: target, int2label dict\n",
    "            single_label: True -> single label prediction else -> predict multiple labels \n",
    "            thres: thres to choose the correct label\n",
    "        return: \n",
    "            predicted labels for test_data\n",
    "        \"\"\"\n",
    "        pred_labels = []\n",
    "        pred_probability = []\n",
    "        for i in range(len(pred_proba)):\n",
    "            if single_label: # single label prediction \n",
    "                pred_pos = pred_proba[i].argmax().item()\n",
    "                pred_val = pred_proba[i].max().item()\n",
    "                pred_probability.append(pred_val)\n",
    "                if (thres is None) or ((thres is not None) and (pred_val >= thres)):\n",
    "                    pred_labels.append(self.get_label_from_dict(pred_pos, use_target_int2label_dict))\n",
    "            else:\n",
    "                multi_labels_ = []\n",
    "                multi_pred_ = []\n",
    "                for j in range(len(pred_proba[i])):\n",
    "                    if (thres is None) or ((thres is not None) and (pred_proba[i][j] >= thres)):\n",
    "                        multi_labels_.append(self.get_label_from_dict(j, use_target_int2label_dict))\n",
    "                        multi_pred_.append(pred_proba[i][j])\n",
    "                pred_labels.append(multi_labels_)\n",
    "                pred_probability.append(multi_pred_)\n",
    "        if pred_proba.shape[0] == 1:\n",
    "            return pred_labels[0], pred_probability[0]\n",
    "        else: \n",
    "            return pred_labels, pred_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfIatGVrnKaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83GwtpE0r50g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wki92f4br7Nv"
   },
   "source": [
    "# Bert Pretrained Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepqa_models import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQ2is1Pir53l"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from pytorch_pretrained_bert import *\n",
    "\n",
    "\n",
    "# PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "#     'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
    "#     'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
    "#     'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
    "#     'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
    "#     'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
    "#     'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
    "#     'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
    "# }\n",
    "# BERT_CONFIG_NAME = 'bert_config.json'\n",
    "# TF_WEIGHTS_NAME = 'model.ckpt'\n",
    "\n",
    "# class BertLayerNorm(nn.Module):\n",
    "#     def __init__(self, hidden_size, eps=1e-12):\n",
    "#         \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).                                         \n",
    "#         \"\"\"\n",
    "#         super(BertLayerNorm, self).__init__()\n",
    "#         self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "#         self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "#         self.variance_epsilon = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         u = x.mean(-1, keepdim=True)\n",
    "#         s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "#         x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "#         return self.weight * x + self.bias\n",
    "\n",
    "# class BertPreTrainedModel(nn.Module):\n",
    "#     \"\"\" An abstract class to handle weights initialization and\n",
    "#         a simple interface for dowloading and loading pretrained models.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config, *inputs, **kwargs):\n",
    "#         super(BertPreTrainedModel, self).__init__()\n",
    "#         if not isinstance(config, BertConfig):\n",
    "#             raise ValueError(\n",
    "#                 \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
    "#                 \"To create a model from a Google pretrained model use \"\n",
    "#                 \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "#                     self.__class__.__name__, self.__class__.__name__\n",
    "#                 ))\n",
    "#         self.config = config\n",
    "\n",
    "#     def init_bert_weights(self, module):\n",
    "#         \"\"\" Initialize the weights.\n",
    "#         \"\"\"\n",
    "#         if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "#             # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "#             # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "#         elif isinstance(module, BertLayerNorm):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1.0)\n",
    "#         if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "#             module.bias.data.zero_()\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
    "#         Download and cache the pre-trained model file if needed.\n",
    "\n",
    "#         Params:\n",
    "#             pretrained_model_name_or_path: either:\n",
    "#                 - a str with the name of a pre-trained model to load selected in the list of:\n",
    "#                     . `bert-base-uncased`\n",
    "#                     . `bert-large-uncased`\n",
    "#                     . `bert-base-cased`\n",
    "#                     . `bert-large-cased`\n",
    "#                     . `bert-base-multilingual-uncased`\n",
    "#                     . `bert-base-multilingual-cased`\n",
    "#                     . `bert-base-chinese`\n",
    "#                 - a path or url to a pretrained model archive containing:\n",
    "#                     . `bert_config.json` a configuration file for the model\n",
    "#                     . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
    "#                 - a path or url to a pretrained model archive containing:\n",
    "#                     . `bert_config.json` a configuration file for the model\n",
    "#                     . `model.chkpt` a TensorFlow checkpoint\n",
    "#             from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n",
    "#             cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
    "#             state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
    "#             *inputs, **kwargs: additional input for the specific Bert class\n",
    "#                 (ex: num_labels for BertForSequenceClassification)\n",
    "#         \"\"\"\n",
    "#         state_dict = kwargs.get('state_dict', None)\n",
    "#         kwargs.pop('state_dict', None)\n",
    "#         cache_dir = kwargs.get('cache_dir', None)\n",
    "#         kwargs.pop('cache_dir', None)\n",
    "#         from_tf = kwargs.get('from_tf', False)\n",
    "#         kwargs.pop('from_tf', None)\n",
    "\n",
    "#         if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
    "#             archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
    "#         else:\n",
    "#             archive_file = pretrained_model_name_or_path\n",
    "#         # redirect to the cache, if necessary\n",
    "#         try:\n",
    "#             resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
    "#         except EnvironmentError:\n",
    "#             logging.error(\n",
    "#                 \"Model name '{}' was not found in model name list ({}). \"\n",
    "#                 \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "#                 \"associated to this path or url.\".format(\n",
    "#                     pretrained_model_name_or_path,\n",
    "#                     ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
    "#                     archive_file))\n",
    "#             return None\n",
    "#         if resolved_archive_file == archive_file:\n",
    "#             logging.info(\"loading archive file {}\".format(archive_file))\n",
    "#         else:\n",
    "#             logging.info(\"loading archive file {} from cache at {}\".format(\n",
    "#                 archive_file, resolved_archive_file))\n",
    "#         tempdir = None\n",
    "#         if os.path.isdir(resolved_archive_file) or from_tf:\n",
    "#             serialization_dir = resolved_archive_file\n",
    "#         else:\n",
    "#             # Extract archive to temp dir\n",
    "#             tempdir = tempfile.mkdtemp()\n",
    "#             logging.info(\"extracting archive file {} to temp dir {}\".format(\n",
    "#                 resolved_archive_file, tempdir))\n",
    "#             with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
    "#                 archive.extractall(tempdir)\n",
    "#             serialization_dir = tempdir\n",
    "#         # Load config\n",
    "#         config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
    "#         if not os.path.exists(config_file):\n",
    "#             # Backward compatibility with old naming format\n",
    "#             config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n",
    "#         config = BertConfig.from_json_file(config_file)\n",
    "#         logging.info(\"Model config {}\".format(config))\n",
    "#         # Instantiate model.\n",
    "#         model = cls(config, *inputs, **kwargs)\n",
    "#         if state_dict is None and not from_tf:\n",
    "#             weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
    "#             state_dict = torch.load(weights_path, map_location='cpu')\n",
    "#         if tempdir:\n",
    "#             # Clean up temp dir\n",
    "#             shutil.rmtree(tempdir)\n",
    "#         if from_tf:\n",
    "#             # Directly load from a TensorFlow checkpoint\n",
    "#             weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n",
    "#             return load_tf_weights_in_bert(model, weights_path)\n",
    "#         # Load from a PyTorch state_dict\n",
    "#         old_keys = []\n",
    "#         new_keys = []\n",
    "#         for key in state_dict.keys():\n",
    "#             new_key = None\n",
    "#             if 'gamma' in key:\n",
    "#                 new_key = key.replace('gamma', 'weight')\n",
    "#             if 'beta' in key:\n",
    "#                 new_key = key.replace('beta', 'bias')\n",
    "#             if new_key:\n",
    "#                 old_keys.append(key)\n",
    "#                 new_keys.append(new_key)\n",
    "#         for old_key, new_key in zip(old_keys, new_keys):\n",
    "#             state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "#         missing_keys = []\n",
    "#         unexpected_keys = []\n",
    "#         error_msgs = []\n",
    "#         # copy state_dict so _load_from_state_dict can modify it\n",
    "#         metadata = getattr(state_dict, '_metadata', None)\n",
    "#         state_dict = state_dict.copy()\n",
    "#         if metadata is not None:\n",
    "#             state_dict._metadata = metadata\n",
    "\n",
    "#         def load(module, prefix=''):\n",
    "#             local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "#             module._load_from_state_dict(\n",
    "#                 state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "#             for name, child in module._modules.items():\n",
    "#                 if child is not None:\n",
    "#                     load(child, prefix + name + '.')\n",
    "#         start_prefix = ''\n",
    "#         if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
    "#             start_prefix = 'bert.'\n",
    "#         load(model, prefix=start_prefix)\n",
    "#         if len(missing_keys) > 0:\n",
    "#             logging.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
    "#                 model.__class__.__name__, missing_keys))\n",
    "#         if len(unexpected_keys) > 0:\n",
    "#             logging.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
    "#                 model.__class__.__name__, unexpected_keys))\n",
    "#         if len(error_msgs) > 0:\n",
    "#             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "#                                model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "#         return model\n",
    "     \n",
    "#     def get_loss(self):\n",
    "#       return self.loss\n",
    "    \n",
    "#     def get_prediction_result(self):\n",
    "#       return self.prediction_result\n",
    "    \n",
    "#     def get_logits(self):\n",
    "#       return self.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JiH_jtVdrk1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lT02_Y06cJDV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21DU2t_Ad8y5"
   },
   "source": [
    "# Bert For Text  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gk4NOJVHdrnx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pytorch\n",
    "\n",
    "class BertForTextClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_labels = 2\n",
    "\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, params):\n",
    "        super(BertForTextClassification, self).__init__(config)\n",
    "        self.n_labels = params['n_labels']\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.n_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        logging.info('text clf: pooled_output {}'.format(pooled_output.shape))\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # final prediction layer\n",
    "        self.logits = self.classifier(pooled_output)\n",
    "        logging.info('text clf: logits {}'.format(self.logits.shape))\n",
    "        \n",
    "        if self.n_labels > 2:\n",
    "          self.prediction_result = F.softmax(self.logits, dim=-1)\n",
    "        else:\n",
    "          self.prediction_result = F.sigmoid(self.logits)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            self.loss = loss_fn(self.logits.view(-1, self.n_labels), labels.view(-1))\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGZ8Cloqdrqp"
   },
   "outputs": [],
   "source": [
    "params = {'n_labels':n_labels, 'label_map':label_map, 'batch_size':16, 'n_epochs':10, 'seq_len':max_seq_len, 'n_workers':-1, 'lr':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5850
    },
    "colab_type": "code",
    "id": "JM2ftZ4tdrtd",
    "outputId": "1ab22fb9-effd-4f6d-f551-a9c3b142927c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz not found in cache, downloading to /tmp/tmpmb4wru1v\n",
      "100%|██████████| 382072689/382072689 [00:32<00:00, 11652271.05B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpmb4wru1v to cache at /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpmb4wru1v\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpptkd5vw6\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForTextClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForTextClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTextClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_clf_model = BertForTextClassification.from_pretrained('bert-base-chinese', params)\n",
    "bert_clf_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ajXhbPfeBpj"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, bert_clf_model.parameters()), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg9gBp3UeBr5"
   },
   "outputs": [],
   "source": [
    "bert_clf_trainer = Trainer(device, batch_size=params['batch_size'], n_epochs=params['n_epochs'], min_clip_val=-1.0, max_clip_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "2kXp0ZZteBuO",
    "outputId": "b3496f7c-b844-4148-fa92-10ab0d819526"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAE3CAYAAAC3ouAOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAwoQFCwAAAADC5L9ydDTclysgnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [29:07<00:00, 172.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n",
      "training   (min:   42.188, max:   92.188, cur:   92.188)\n",
      "validation (min:   84.375, max:  100.000, cur:   98.438)\n",
      "\n",
      "loss:\n",
      "training   (min:    0.227, max:    1.838, cur:    0.227)\n",
      "validation (min:    0.055, max:    0.557, cur:    0.055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_clf_trainer.train_and_evaluate(bert_clf_model, train_data, val_data, optimizer, metrics, loss_fn=None, model_dir='./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KNhy8e8eBwn"
   },
   "outputs": [],
   "source": [
    "bert_clf_pred = Predictor(device, model=bert_clf_model, max_seq_length=max_seq_len, tokenizer=tokenizer, X_proc=X_proc, target_int2label_dict=y_mvec.int2label_dict,\n",
    "                         target_label2int_dict=y_mvec.label2int_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ijOgp-il0L8"
   },
   "outputs": [],
   "source": [
    "pred_label, pred_probability, probability = bert_clf_pred.predict(df=text_clf_df)\n",
    "#print(pred_label, pred_probability, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rC_uBXkNSj4T",
    "outputId": "ab6b9fdc-cdec-4e9f-dfc9-39e668089db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3885,)\n"
     ]
    }
   ],
   "source": [
    "test_label = text_clf_df['label'].values\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rVslmSppV6hN",
    "outputId": "aee1ad92-fa94-4425-db0a-008303d1f55a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['客户抱怨', '施压警告', '无还款能力', '有意愿还款', '核资', '表明来意'], dtype='<U5')"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Sg-aTAGeiKJe",
    "outputId": "bd7c8c2b-23f4-4513-899f-dbfed6f7eda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['表明来意' '有意愿还款' '表明来意' '表明来意' '核资' '有意愿还款' '表明来意' '有助于还款' '核资' '核资'] \n",
      " ['施压警告', '客户抱怨', '核资', '有意愿还款', '表明来意', '核资', '表明来意', '无还款能力', '客户抱怨', '施压警告']\n"
     ]
    }
   ],
   "source": [
    "print(test_label[:10],'\\n', pred_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgbFt65NTJeG"
   },
   "outputs": [],
   "source": [
    "# import scikitplot as skplot\n",
    "\n",
    "# skplot.metrics.plot_confusion_matrix(test_label, pred_label, normalize=True, figsize=(8,6))\n",
    "# skplot.metrics.plot_roc(test_label, probability.cpu(), figsize=(8,6))\n",
    "# skplot.metrics.plot_precision_recall(test_label, probability.cpu(), figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1n1N3EqmEwu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9VkoCDPmEzu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epUGCfN3eDyL"
   },
   "source": [
    "# Bert For Sequence Label (NER Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJNX1BgdeBzA"
   },
   "outputs": [],
   "source": [
    "class BertForNER(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for token-level classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the full hidden state of the last layer.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_labels = 2\n",
    "\n",
    "    model = BertForTokenClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, params):\n",
    "        super(BertForNER, self).__init__(config)\n",
    "        self.n_labels = params['n_labels']\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.n_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        logging.info('bert ner: sequence_output shape {}'.format(sequence_output.shape))\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        # final prediction layer\n",
    "        self.logits = self.classifier(sequence_output)\n",
    "        logging.info('bert ner: logits shape {}'.format(self.logits.shape))\n",
    "        \n",
    "        # to do: add crf layer\n",
    "        \n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                self.loss = loss_fn(active_logits, active_labels)\n",
    "            else:\n",
    "                self.loss = loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9kN6vJpeB1j"
   },
   "outputs": [],
   "source": [
    "params = {'n_labels':n_labels, 'label_map':label_map, 'batch_size':16, 'n_epochs':10, 'seq_len':max_seq_len, 'n_workers':-1, 'lr':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QWtu1h6eCV_"
   },
   "outputs": [],
   "source": [
    "bert_ner_model = BertForTextClassification.from_pretrained('bert-base-chinese', params)\n",
    "bert_ner_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQISPrAheCY-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCK9O7C0eCbb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6IId8r_eCdh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7PfTUQjeCgF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i356UQoseckX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_NS4xm7edq0"
   },
   "source": [
    "# Bert For Entity Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "lxI8W-YRecm9",
    "outputId": "f6f4afa3-1c04-4e8c-f595-065043296ca3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-448f6e17da4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertForEntityRelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m   \u001b[0mBert\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mEntity\u001b[0m \u001b[0mRelation\u001b[0m \u001b[0mExtraction\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \"\"\"\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertPreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "class BertForEntityRelation(BertPreTrainedModel):\n",
    "  \"\"\"\n",
    "  Bert for Entity Relation Extraction Task\n",
    "  \"\"\"\n",
    "  def __init__(self, config, params):\n",
    "    super(BertForEntityRelation, self).__init__(config)\n",
    "    self.n_labels = params['n_labels']\n",
    "    \n",
    "    self.bert = BertModel(config)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size, self.n_labels)\n",
    "    self.apply(self.init_bert_weights)\n",
    "    \n",
    "    \n",
    "  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        logging.info('bert phrase sim: pooled_output shape {}'.format(pooled_output))\n",
    "        #seq_relationship_score = self.cls( pooled_output)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # final prediction layer\n",
    "        self.logits = self.classifier(pooled_output)\n",
    "        logging.info('text clf: logits {}'.format(self.logits.shape))\n",
    "        \n",
    "        if self.n_labels > 2:\n",
    "          self.prediction_result = F.softmax(self.logits, dim=-1)\n",
    "        else:\n",
    "          self.prediction_result = F.sigmoid(self.logits)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            self.loss = loss_fn(self.logits.view(-1, self.n_labels), labels.view(-1))\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "colab_type": "code",
    "id": "uiIC2Gp3ecpZ",
    "outputId": "9980ec39-d0bb-4279-a98d-4562075f773f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3d12e0ff1229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_labels'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label_map'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_workers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_labels' is not defined"
     ]
    }
   ],
   "source": [
    "params = {'n_labels':n_labels, 'label_map':label_map, 'batch_size':16, 'n_epochs':10, 'seq_len':max_seq_len, 'n_workers':-1, 'lr':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bbpum7mYecsB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vy5aapvhecuP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbhgVsWseCim"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_EUGAHIeClD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHxTLzloeL8_"
   },
   "source": [
    "# Bert For Phrase Sim Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qvx9i7veKNA"
   },
   "outputs": [],
   "source": [
    "class BertForPhraseSim(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Bert For Phrase similarity Task\n",
    "    \"\"\"\n",
    "    def __init__(self, config, params):\n",
    "        super(BertForPhraseSim, self).__init__(config)\n",
    "        self.n_labels = params['n_labels']\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.n_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        logging.info('bert phrase sim: pooled_output shape {}'.format(pooled_output))\n",
    "        #seq_relationship_score = self.cls( pooled_output)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # final prediction layer\n",
    "        self.logits = self.classifier(pooled_output)\n",
    "        logging.info('text clf: logits {}'.format(self.logits.shape))\n",
    "        \n",
    "        if self.n_labels > 2:\n",
    "          self.prediction_result = F.softmax(self.logits, dim=-1)\n",
    "        else:\n",
    "          self.prediction_result = F.sigmoid(self.logits)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            self.loss = loss_fn(self.logits.view(-1, self.n_labels), labels.view(-1))\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ufa_u_-eKPr"
   },
   "outputs": [],
   "source": [
    "ps_proc = TextPhraseSimProcessor(labels=None, feature_columns=['passage', 'question'], label_columns='is_selected')\n",
    "\n",
    "train_examples = ps_proc.get_train_examples(df=ucac_train_df, size=1000, labels_available=True)\n",
    "dev_examples = ps_proc.get_dev_examples(df=ucac_train_df, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nWaem6CjugKv",
    "outputId": "d9b51763-babf-4fad-b214-27abfbd24356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n"
     ]
    }
   ],
   "source": [
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36872
    },
    "colab_type": "code",
    "id": "rmV7-OeVeKSM",
    "outputId": "27b9a3ac-9150-4a8a-8032-dcad7ee22dfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 1000\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 991276\n",
      "INFO:root:tokens: [CLS] 黑 芝 麻 到 底 可 以 和 蜂 蜜 一 起 吃 吗 黑 芝 麻 是 一 种 非 常 不 错 的 补 品 ， 老 年 人 经 常 吃 一 些 黑 芝 麻 可 以 有 效 的 调 节 我 们 自 身 出 现 的 白 头 发 的 问 题 ， 帮 助 我 们 去 除 黑 色 素 。 经 常 性 的 吃 一 些 蜂 蜜 ， 对 于 我 们 自 身 的 嗓 子 也 会 有 很 大 的 好 处 ， 还 可 以 帮 助 领 略 蜂 蜜 的 美 味 ， 详 细 的 为 大 家 介 绍 一 下 黑 芝 麻 可 以 和 蜂 蜜 一 起 吃 吗 呢 。 [SEP] 黑 芝 麻 可 以 放 蜂 蜜 吗 [SEP]\n",
      "INFO:root:input_ids: 101 7946 5698 7937 1168 2419 1377 809 1469 6044 6057 671 6629 1391 1408 7946 5698 7937 3221 671 4905 7478 2382 679 7231 4638 6133 1501 8024 5439 2399 782 5307 2382 1391 671 763 7946 5698 7937 1377 809 3300 3126 4638 6444 5688 2769 812 5632 6716 1139 4385 4638 4635 1928 1355 4638 7309 7579 8024 2376 1221 2769 812 1343 7370 7946 5682 5162 511 5307 2382 2595 4638 1391 671 763 6044 6057 8024 2190 754 2769 812 5632 6716 4638 1624 2094 738 833 3300 2523 1920 4638 1962 1905 8024 6820 1377 809 2376 1221 7566 4526 6044 6057 4638 5401 1456 8024 6422 5301 4638 711 1920 2157 792 5305 671 678 7946 5698 7937 1377 809 1469 6044 6057 671 6629 1391 1408 1450 511 102 7946 5698 7937 1377 809 3123 6044 6057 1408 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1330051\n",
      "INFO:root:tokens: [CLS] 我 下 午 躺 在 床 上 看 手 机 的 时 候 突 然 翻 个 身 就 开 我 下 午 躺 在 床 上 看 手 机 的 时 候 突 然 翻 个 身 就 开 始 头 昏 ， 就 有 点 天 旋 地 转 那 种 ， 不 过 也 没 那 么 严 重 ， 躺 了 一 会 儿 起 床 发 现 走 路 也 在 晃 ， 走 不 稳 。 去 上 厕 所 开 门 被 风 吹 得 差 点 没 站 稳 。 后 来 休 息 一 小 时 左 右 慢 慢 恢 复 。 晚 上 洗 澡 的 时 候 洗 头 ， 又 突 然 昏 起 来 ， 差 点 摔 了 。 之 前 一 个 月 内 有 过 心 口 一 呼 吸 狠 了 就 痛 的 症 状 。 我 也 有 点 焦 躁 症 。 想 问 问 这 是 怎 么 了 ？ 是 身 体 不 好 还 是 心 理 病 引 起 得 ？ [SEP] 躺 会 儿 猛 然 起 来 感 觉 有 点 昏 怎 么 了 [SEP]\n",
      "INFO:root:input_ids: 101 2769 678 1286 6720 1762 2414 677 4692 2797 3322 4638 3198 952 4960 4197 5436 702 6716 2218 2458 2769 678 1286 6720 1762 2414 677 4692 2797 3322 4638 3198 952 4960 4197 5436 702 6716 2218 2458 1993 1928 3210 8024 2218 3300 4157 1921 3181 1765 6760 6929 4905 8024 679 6814 738 3766 6929 720 698 7028 8024 6720 749 671 833 1036 6629 2414 1355 4385 6624 6662 738 1762 3230 8024 6624 679 4937 511 1343 677 1329 2792 2458 7305 6158 7599 1430 2533 2345 4157 3766 4991 4937 511 1400 3341 828 2622 671 2207 3198 2340 1381 2714 2714 2612 1908 511 3241 677 3819 4074 4638 3198 952 3819 1928 8024 1348 4960 4197 3210 6629 3341 8024 2345 4157 3035 749 511 722 1184 671 702 3299 1079 3300 6814 2552 1366 671 1461 1429 4321 749 2218 4578 4638 4568 4307 511 2769 738 3300 4157 4193 6708 4568 511 2682 7309 7309 6821 3221 2582 720 749 8043 3221 6716 860 679 1962 6820 3221 2552 4415 4567 2471 6629 2533 8043 102 6720 833 1036 4338 4197 6629 3341 2697 6230 3300 4157 3210 2582 720 749 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 663639\n",
      "INFO:root:tokens: [CLS] android 手 机 怎 么 退 出 安 全 模 式 呵 呵 。 刚 才 我 也 成 这 样 了 ， 然 后 解 决 了 哦 。 恢 复 出 厂 设 置 没 用 。 手 机 关 机 后 ， 把 电 池 扣 下 来 再 重 新 装 上 。 然 后 开 机 时 会 出 现 samsung 字 样 ， 然 后 按 住 home 键 ， 不 放 手 ， 大 约 8 秒 即 可 。 如 果 不 放 心 ， 按 到 开 机 也 行 。 然 后 开 机 后 ， 安 全 模 式 就 没 了 。 【 最 先 我 也 打 不 开 下 载 的 程 序 ， 只 能 打 电 话 】 一 般 安 全 模 式 都 是 因 为 手 机 系 统 突 然 故 障 【 如 进 水 ， 经 常 死 机 ， 手 机 发 烫 ， 中 病 毒 ， 键 盘 故 障 等 等 】 如 果 还 是 不 行 ， 你 就 去 售 后 吧 。 【 1 . 维 修 手 机 。 2 . 刷 机 】 不 过 我 弄 成 功 了 。 现 在 是 来 解 决 关 于 安 全 模 式 的 问 题 的 。 [SEP] 安 卓 手 机 怎 么 去 掉 安 全 模 式 [SEP]\n",
      "INFO:root:input_ids: 101 8254 2797 3322 2582 720 6842 1139 2128 1059 3563 2466 1457 1457 511 1157 2798 2769 738 2768 6821 3416 749 8024 4197 1400 6237 1104 749 1521 511 2612 1908 1139 1322 6392 5390 3766 4500 511 2797 3322 1068 3322 1400 8024 2828 4510 3737 2807 678 3341 1086 7028 3173 6163 677 511 4197 1400 2458 3322 3198 833 1139 4385 8931 2099 3416 8024 4197 1400 2902 857 8563 7241 8024 679 3123 2797 8024 1920 5276 129 4907 1315 1377 511 1963 3362 679 3123 2552 8024 2902 1168 2458 3322 738 6121 511 4197 1400 2458 3322 1400 8024 2128 1059 3563 2466 2218 3766 749 511 523 3297 1044 2769 738 2802 679 2458 678 6770 4638 4923 2415 8024 1372 5543 2802 4510 6413 524 671 5663 2128 1059 3563 2466 6963 3221 1728 711 2797 3322 5143 5320 4960 4197 3125 7397 523 1963 6822 3717 8024 5307 2382 3647 3322 8024 2797 3322 1355 4176 8024 704 4567 3681 8024 7241 4669 3125 7397 5023 5023 524 1963 3362 6820 3221 679 6121 8024 872 2218 1343 1545 1400 1416 511 523 122 119 5335 934 2797 3322 511 123 119 1170 3322 524 679 6814 2769 2462 2768 1216 749 511 4385 1762 3221 3341 6237 1104 1068 754 2128 1059 3563 2466 4638 7309 7579 4638 511 102 2128 1294 2797 3322 2582 720 1343 2957 2128 1059 3563 2466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 1 (id = 1)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1662722\n",
      "INFO:root:tokens: [CLS] 跨 文 化 交 际 中 英 汉 日 常 礼 貌 用 语 的 语 用 差 异 2 . 中 方 礼 貌 原 则 汉 语 文 化 中 的 [UNK] 礼 [UNK] 和 [UNK] 礼 貌 [UNK] 有 别 于 古 代 汉 语 中 的 [UNK] 礼 [UNK] 。 这 里 的 礼 貌 相 当 于 现 代 汉 语 中 [UNK] 礼 貌 待 人 [UNK] 和 [UNK] 礼 貌 行 车 [UNK] 中 的 礼 貌 ， 是 制 约 现 在 人 说 话 、 行 事 的 一 套 行 为 规 范 。 对 现 代 汉 语 中 的 礼 貌 做 过 系 统 的 研 究 的 当 首 推 顾 曰 国 ， 本 文 汉 语 礼 貌 用 语 规 则 以 他 的 观 点 为 主 要 依 据 。 他 在 1990 年 提 出 汉 语 礼 貌 4 准 则 ： 自 贬 准 则 、 称 呼 准 则 、 策 略 及 慷 慨 准 则 、 对 等 及 真 诚 准 则 。 [SEP] 跨 文 化 交 际 中 中 英 礼 貌 语 差 异 [SEP]\n",
      "INFO:root:input_ids: 101 6659 3152 1265 769 7354 704 5739 3727 3189 2382 4851 6505 4500 6427 4638 6427 4500 2345 2460 123 119 704 3175 4851 6505 1333 1156 3727 6427 3152 1265 704 4638 100 4851 100 1469 100 4851 6505 100 3300 1166 754 1367 807 3727 6427 704 4638 100 4851 100 511 6821 7027 4638 4851 6505 4685 2496 754 4385 807 3727 6427 704 100 4851 6505 2521 782 100 1469 100 4851 6505 6121 6756 100 704 4638 4851 6505 8024 3221 1169 5276 4385 1762 782 6432 6413 510 6121 752 4638 671 1947 6121 711 6226 5745 511 2190 4385 807 3727 6427 704 4638 4851 6505 976 6814 5143 5320 4638 4777 4955 4638 2496 7674 2972 7560 3288 1744 8024 3315 3152 3727 6427 4851 6505 4500 6427 6226 1156 809 800 4638 6225 4157 711 712 6206 898 2945 511 800 1762 8431 2399 2990 1139 3727 6427 4851 6505 125 1114 1156 8038 5632 6578 1114 1156 510 4917 1461 1114 1156 510 5032 4526 1350 2724 2717 1114 1156 510 2190 5023 1350 4696 6411 1114 1156 511 102 6659 3152 1265 769 7354 704 704 5739 4851 6505 6427 2345 2460 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1830625\n",
      "INFO:root:tokens: [CLS] 防 晒 霜 隔 多 长 时 间 涂 一 次 最 好 ？ 炎 炎 夏 日 下 ， 很 多 人 都 随 身 携 带 着 防 晒 霜 ， 但 是 在 使 用 时 ， 有 的 人 一 天 只 涂 抹 一 次 ， 有 的 人 则 不 停 地 往 身 上 涂 抹 ， 防 晒 霜 究 竟 多 久 抹 一 次 才 科 学 ？ 解 放 军 304 医 院 皮 肤 科 主 任 邹 先 彪 大 夫 告 诉 记 者 ， 只 有 了 解 了 防 晒 品 中 的 一 些 技 术 指 标 ， 才 能 科 学 地 使 用 它 来 护 肤 [SEP] 防 晒 霜 应 该 什 么 时 候 涂 [SEP]\n",
      "INFO:root:input_ids: 101 7344 3235 7458 7392 1914 7270 3198 7313 3864 671 3613 3297 1962 8043 4142 4142 1909 3189 678 8024 2523 1914 782 6963 7390 6716 3025 2372 4708 7344 3235 7458 8024 852 3221 1762 886 4500 3198 8024 3300 4638 782 671 1921 1372 3864 2851 671 3613 8024 3300 4638 782 1156 679 977 1765 2518 6716 677 3864 2851 8024 7344 3235 7458 4955 4994 1914 719 2851 671 3613 2798 4906 2110 8043 6237 3123 1092 9627 1278 7368 4649 5502 4906 712 818 6941 1044 2507 1920 1923 1440 6401 6381 5442 8024 1372 3300 749 6237 749 7344 3235 1501 704 4638 671 763 2825 3318 2900 3403 8024 2798 5543 4906 2110 1765 886 4500 2124 3341 2844 5502 102 7344 3235 7458 2418 6421 784 720 3198 952 3864 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 1000\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 715161\n",
      "INFO:root:tokens: [CLS] 己 未 日 ， 己 未 日 柱 命 理 分 析 3 ） 因 此 ， 己 未 日 出 生 的 男 性 ， 有 出 生 于 大 家 族 之 兆 ， 在 人 生 路 上 能 有 大 发 展 。 若 从 事 工 商 ， 定 有 分 厂 、 分 公 司 或 者 分 店 。 所 娶 的 妻 子 ， 也 很 贤 淑 。 4 ） 己 未 日 出 生 的 女 性 ， 其 父 亲 的 运 气 很 有 可 能 不 好 。 己 未 日 个 人 则 会 在 结 婚 后 运 气 逐 渐 走 好 。 但 是 容 易 产 生 感 情 困 扰 和 苦 恼 。 己 未 日 出 生 的 女 性 ， 容 易 和 有 过 婚 恋 史 的 男 性 结 婚 。 也 容 易 和 婚 恋 关 系 尚 且 存 续 的 男 士 发 生 感 情 纠 葛 。 所 以 希 望 己 未 日 出 生 的 女 性 ， 面 对 男 女 感 情 时 要 多 一 份 谨 慎 。 5 ） 但 是 ， 总 体 说 来 ， 己 未 日 出 生 的 人 ， 无 论 男 女 ， 都 有 为 人 稳 重 的 一 面 。 处 事 有 分 寸 ， 自 重 ， 具 有 进 取 心 。 没 有 非 分 的 欲 望 ， 但 处 世 不 免 有 猜 疑 心 。 因 此 ， 在 特 定 的 时 空 条 件 下 ， 出 现 自 讨 苦 吃 的 尴 尬 。 6 ） 己 未 日 出 生 的 人 ， 无 论 男 女 ， 和 属 蛇 、 属 马 的 人 ， 容 易 走 到 一 起 。 但 若 真 有 这 样 的 友 谊 出 现 ， 则 会 让 己 未 日 出 生 的 人 ， 更 加 自 我 封 闭 ， 更 加 缺 乏 生 发 、 创 造 之 意 ， 7 ） 己 未 日 出 生 的 人 ， 无 论 男 女 ， 和 属 猪 、 属 兔 的 人 ， 也 容 易 [SEP] 己 未 日 和 戊 戌 日 合 吗 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:input_ids: 101 2346 3313 3189 8024 2346 3313 3189 3393 1462 4415 1146 3358 124 8021 1728 3634 8024 2346 3313 3189 1139 4495 4638 4511 2595 8024 3300 1139 4495 754 1920 2157 3184 722 1042 8024 1762 782 4495 6662 677 5543 3300 1920 1355 2245 511 5735 794 752 2339 1555 8024 2137 3300 1146 1322 510 1146 1062 1385 2772 5442 1146 2421 511 2792 2034 4638 1988 2094 8024 738 2523 6570 3902 511 125 8021 2346 3313 3189 1139 4495 4638 1957 2595 8024 1071 4266 779 4638 6817 3698 2523 3300 1377 5543 679 1962 511 2346 3313 3189 702 782 1156 833 1762 5310 2042 1400 6817 3698 6852 3933 6624 1962 511 852 3221 2159 3211 772 4495 2697 2658 1737 2817 1469 5736 2630 511 2346 3313 3189 1139 4495 4638 1957 2595 8024 2159 3211 1469 3300 6814 2042 2605 1380 4638 4511 2595 5310 2042 511 738 2159 3211 1469 2042 2605 1068 5143 2213 684 2100 5330 4638 4511 1894 1355 4495 2697 2658 5272 5867 511 2792 809 2361 3307 2346 3313 3189 1139 4495 4638 1957 2595 8024 7481 2190 4511 1957 2697 2658 3198 6206 1914 671 819 6474 2708 511 126 8021 852 3221 8024 2600 860 6432 3341 8024 2346 3313 3189 1139 4495 4638 782 8024 3187 6389 4511 1957 8024 6963 3300 711 782 4937 7028 4638 671 7481 511 1905 752 3300 1146 2189 8024 5632 7028 8024 1072 3300 6822 1357 2552 511 3766 3300 7478 1146 4638 3617 3307 8024 852 1905 686 679 1048 3300 4339 4542 2552 511 1728 3634 8024 1762 4294 2137 4638 3198 4958 3340 816 678 8024 1139 4385 5632 6374 5736 1391 4638 2219 2217 511 127 8021 2346 3313 3189 1139 4495 4638 782 8024 3187 6389 4511 1957 8024 1469 2247 6026 510 2247 7716 4638 782 8024 2159 3211 6624 1168 671 6629 511 852 5735 4696 3300 6821 3416 4638 1351 6449 1139 4385 8024 1156 833 6375 2346 3313 3189 1139 4495 4638 782 8024 3291 1217 5632 2769 2196 7308 8024 3291 1217 5375 726 4495 1355 510 1158 6863 722 2692 8024 128 8021 2346 3313 3189 1139 4495 4638 782 8024 3187 6389 4511 1957 8024 1469 2247 4343 510 2247 1052 4638 782 8024 738 2159 3211 102 2346 3313 3189 1469 2763 2764 3189 1394 1408 102\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 893855\n",
      "INFO:root:tokens: [CLS] 新 电 脑 笔 记 本 如 何 验 机 ， 小 心 商 家 偷 换 配 置 ！ 屏 幕 检 测 ： 记 得 点 击 右 下 角 的 屏 幕 检 测 ， 这 个 功 能 可 以 让 你 准 确 看 到 屏 幕 是 否 有 坏 点 ， 但 是 记 住 不 要 太 过 于 纠 结 屏 幕 漏 光 的 问 题 ， 那 是 因 为 电 脑 模 具 本 身 不 够 精 细 导 致 的 ， 如 果 因 为 漏 光 而 换 机 ， 那 么 换 来 的 笔 记 本 说 不 定 漏 光 比 之 前 的 更 严 重 。 [SEP] 卖 家 换 笔 记 本 硬 件 [SEP]\n",
      "INFO:root:input_ids: 101 3173 4510 5554 5011 6381 3315 1963 862 7741 3322 8024 2207 2552 1555 2157 982 2940 6981 5390 8013 2242 2391 3466 3844 8038 6381 2533 4157 1140 1381 678 6235 4638 2242 2391 3466 3844 8024 6821 702 1216 5543 1377 809 6375 872 1114 4802 4692 1168 2242 2391 3221 1415 3300 1776 4157 8024 852 3221 6381 857 679 6206 1922 6814 754 5272 5310 2242 2391 4026 1045 4638 7309 7579 8024 6929 3221 1728 711 4510 5554 3563 1072 3315 6716 679 1916 5125 5301 2193 5636 4638 8024 1963 3362 1728 711 4026 1045 5445 2940 3322 8024 6929 720 2940 3341 4638 5011 6381 3315 6432 679 2137 4026 1045 3683 722 1184 4638 3291 698 7028 511 102 1297 2157 2940 5011 6381 3315 4801 816 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1177926\n",
      "INFO:root:tokens: [CLS] 什 么 是 期 货 ？ 怎 么 玩 ？ 期 货 是 现 在 进 行 买 卖 ， 但 是 在 将 来 进 行 交 收 或 交 割 的 标 的 物 ， 这 个 标 的 物 可 以 是 某 种 商 品 例 如 黄 金 、 原 油 、 农 产 品 ， 也 可 以 是 金 融 工 具 ， 还 可 以 是 金 融 指 标 。 交 收 期 货 的 日 子 可 以 是 一 星 期 之 后 ， 一 个 月 之 后 ， 三 个 月 之 后 ， 甚 至 一 年 之 后 。 [SEP] 期 货 怎 么 玩 [SEP]\n",
      "INFO:root:input_ids: 101 784 720 3221 3309 6573 8043 2582 720 4381 8043 3309 6573 3221 4385 1762 6822 6121 743 1297 8024 852 3221 1762 2199 3341 6822 6121 769 3119 2772 769 1200 4638 3403 4638 4289 8024 6821 702 3403 4638 4289 1377 809 3221 3378 4905 1555 1501 891 1963 7942 7032 510 1333 3779 510 1093 772 1501 8024 738 1377 809 3221 7032 6084 2339 1072 8024 6820 1377 809 3221 7032 6084 2900 3403 511 769 3119 3309 6573 4638 3189 2094 1377 809 3221 671 3215 3309 722 1400 8024 671 702 3299 722 1400 8024 676 702 3299 722 1400 8024 4493 5635 671 2399 722 1400 511 102 3309 6573 2582 720 4381 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 324830\n",
      "INFO:root:tokens: [CLS] 机 油 3 个 月 换 一 次 ， 那 变 速 箱 油 多 久 换 一 次 ？ 听 听 专 家 怎 么 说 ！ 许 多 车 主 都 搞 不 清 变 速 箱 油 到 底 需 不 要 换 ？ 不 换 能 怎 么 样 ？ 如 果 需 要 换 ， 到 底 多 久 换 一 次 更 合 理 ？ 什 么 情 况 下 因 为 变 速 箱 问 题 会 导 致 汽 车 提 前 报 废 ？ 变 速 箱 作 为 独 立 系 统 极 其 重 要 的 机 械 式 组 成 部 分 ， 可 以 改 变 传 动 比 起 到 驱 动 作 用 。 变 速 箱 油 是 用 来 清 理 排 挡 系 统 的 ， 正 确 定 期 更 换 可 以 起 到 维 护 变 速 箱 正 常 工 作 以 及 减 少 磨 损 的 作 用 。 机 油 3 个 月 换 一 次 ， 那 变 速 箱 油 多 久 换 一 次 ？ 听 听 专 家 怎 么 说 ！ [SEP] 变 速 箱 机 油 多 久 换 一 次 [SEP]\n",
      "INFO:root:input_ids: 101 3322 3779 124 702 3299 2940 671 3613 8024 6929 1359 6862 5056 3779 1914 719 2940 671 3613 8043 1420 1420 683 2157 2582 720 6432 8013 6387 1914 6756 712 6963 3018 679 3926 1359 6862 5056 3779 1168 2419 7444 679 6206 2940 8043 679 2940 5543 2582 720 3416 8043 1963 3362 7444 6206 2940 8024 1168 2419 1914 719 2940 671 3613 3291 1394 4415 8043 784 720 2658 1105 678 1728 711 1359 6862 5056 7309 7579 833 2193 5636 3749 6756 2990 1184 2845 2426 8043 1359 6862 5056 868 711 4324 4989 5143 5320 3353 1071 7028 6206 4638 3322 3462 2466 5299 2768 6956 1146 8024 1377 809 3121 1359 837 1220 3683 6629 1168 7721 1220 868 4500 511 1359 6862 5056 3779 3221 4500 3341 3926 4415 2961 2913 5143 5320 4638 8024 3633 4802 2137 3309 3291 2940 1377 809 6629 1168 5335 2844 1359 6862 5056 3633 2382 2339 868 809 1350 1121 2208 4836 2938 4638 868 4500 511 3322 3779 124 702 3299 2940 671 3613 8024 6929 1359 6862 5056 3779 1914 719 2940 671 3613 8043 1420 1420 683 2157 2582 720 6432 8013 102 1359 6862 5056 3322 3779 1914 719 2940 671 3613 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1194406\n",
      "INFO:root:tokens: [CLS] hpv 病 毒 的 治 疗 ， 什 么 方 法 好 你 好 ， 在 性 生 活 中 ， 男 方 若 患 有 尖 锐 湿 疣 ， 加 上 不 戴 套 ， 很 容 易 将 尖 锐 湿 疣 传 染 给 女 性 ， 即 便 是 有 安 全 套 ， 若 男 方 手 上 长 有 尖 锐 湿 疣 ， 可 能 会 在 抚 摸 女 性 阴 道 的 时 候 ， 将 尖 锐 湿 疣 传 染 给 女 方 。 所 以 交 叉 感 染 是 尖 锐 湿 疣 疾 病 扩 散 的 主 要 诱 因 ， 尖 锐 湿 疣 的 发 病 病 灶 和 发 病 时 间 和 个 人 病 毒 定 量 对 于 该 病 的 治 疗 是 很 关 键 的 。 [SEP] hpv 最 好 的 治 疗 方 法 [SEP]\n",
      "INFO:root:input_ids: 101 11067 4567 3681 4638 3780 4545 8024 784 720 3175 3791 1962 872 1962 8024 1762 2595 4495 3833 704 8024 4511 3175 5735 2642 3300 2211 7229 3969 4551 8024 1217 677 679 2785 1947 8024 2523 2159 3211 2199 2211 7229 3969 4551 837 3381 5314 1957 2595 8024 1315 912 3221 3300 2128 1059 1947 8024 5735 4511 3175 2797 677 7270 3300 2211 7229 3969 4551 8024 1377 5543 833 1762 2836 3043 1957 2595 7346 6887 4638 3198 952 8024 2199 2211 7229 3969 4551 837 3381 5314 1957 3175 511 2792 809 769 1349 2697 3381 3221 2211 7229 3969 4551 4565 4567 2810 3141 4638 712 6206 6430 1728 8024 2211 7229 3969 4551 4638 1355 4567 4567 4131 1469 1355 4567 3198 7313 1469 702 782 4567 3681 2137 7030 2190 754 6421 4567 4638 3780 4545 3221 2523 1068 7241 4638 511 102 11067 3297 1962 4638 3780 4545 3175 3791 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "label_map = {0:0, 1:1}\n",
    "\n",
    "train_data = get_batch(train_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)\n",
    "val_data = get_batch(dev_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mQ_jJxYeShE"
   },
   "outputs": [],
   "source": [
    "params = {'n_labels':2, 'label_map':label_map, 'batch_size':16, 'n_epochs':10, 'seq_len':max_seq_len, 'n_workers':-1, 'lr':0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5850
    },
    "colab_type": "code",
    "id": "xyWv2VwYeSjv",
    "outputId": "94cd9584-25b3-4c40-cc7c-f0d87a9d7cee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz not found in cache, downloading to /tmp/tmpouc85y9q\n",
      "100%|██████████| 382072689/382072689 [00:33<00:00, 11244285.38B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpouc85y9q to cache at /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpouc85y9q\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.cache/torch/pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpxvl6ig1t\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForPhraseSim not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForPhraseSim: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPhraseSim(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_phrase_model = BertForPhraseSim.from_pretrained('bert-base-chinese', params)\n",
    "bert_phrase_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWZPGatqeSmU"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, bert_phrase_model.parameters()), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gabJa950eSo0"
   },
   "outputs": [],
   "source": [
    "bert_phrase_trainer = Trainer(device, batch_size=params['batch_size'], n_epochs=params['n_epochs'], min_clip_val=-1.0, max_clip_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "Se_Ux__SeKXR",
    "outputId": "6100dc09-b1b2-4da6-ce96-f731646eb157"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]INFO:root:Epoch 1/10\n",
      "\n",
      "train_data_iter:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8f38edde5d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_phrase_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_phrase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-f8a9e0a91cf8>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self, model, train_data, dev_data, optimizer, metrics, loss_fn, model_dir)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f8a9e0a91cf8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_data, optimizer, sum_writer, epoch, metrics, loss_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# compute model output and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#print('train loss shape: ', loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-9c27bee79f47>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n\u001b[0;32m---> 23\u001b[0;31m                                      output_all_encoded_layers=False)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert phrase sim: pooled_output shape {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#seq_relationship_score = self.cls( pooled_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    743\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    744\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/pytorch-pretrained-BERT/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.17 GiB total capacity; 10.63 GiB already allocated; 512.00 KiB free; 217.38 MiB cached)"
     ]
    }
   ],
   "source": [
    "bert_phrase_trainer.train_and_evaluate(bert_phrase_model, train_data, val_data, optimizer, metrics, loss_fn=None, model_dir='./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCMM7k8fyX4X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hsZUKDpWENu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eh4daBAQWEhn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LCN1p5yWE0X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbjecfsKWH7r"
   },
   "source": [
    "# Bert Aspect Term Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMWAVX0gWFYr"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertForSequenceClassification, BertModel\n",
    "\n",
    "class BertForAspect(BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Bert For aspect term Task\n",
    "    \"\"\"\n",
    "    def __init__(self, config, params):\n",
    "        super(BertForAspect, self).__init__(config, params['n_labels'])\n",
    "        self.n_labels = params['n_labels']\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.n_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "        self.multiheads = MultiHeadAttention(config.hidden_size, params['heads'], keep_prob=params['keep_prob'])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        #logging.info('bert phrase sim: pooled_output shape {}'.format(pooled_output))\n",
    "        logging.info('bert for aspect {}'.format(sequence_output.shape))    \n",
    "\n",
    "        pooled_output = self.dropout(sequence_output)\n",
    "        pooled_output = self.multiheads(pooled_output)\n",
    "        logging.info('multihead pooled_output shape {}'.format(pooled_output.shape))\n",
    "        \n",
    "        # final prediction layer\n",
    "        self.logits = self.classifier(pooled_output)\n",
    "        logging.info('text clf: logits {}'.format(self.logits.shape))\n",
    "        \n",
    "        if self.n_labels > 2:\n",
    "          self.prediction_result = F.softmax(self.logits, dim=-1)\n",
    "        else:\n",
    "          self.prediction_result = F.sigmoid(self.logits)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            self.loss = loss_fn(self.logits.view(-1, self.n_labels), labels.view(-1))\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n",
    "\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.loss\n",
    "    \n",
    "    def get_prediction_result(self):\n",
    "        return self.prediction_result\n",
    "    \n",
    "    def get_logits(self):\n",
    "        return self.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1ksLwzqa37K"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, keep_prob=0.1):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.dropout = nn.Dropout(self.keep_prob)\n",
    "        \n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: d_k\n",
    "            key: d_k\n",
    "            value: d_v\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        logging.info('scores shape {}'.format(scores.shape))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0,-1e9)\n",
    "        p_attn = F.softmax(scores,dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            p_attn = self.dropout(p_attn)\n",
    "        return torch.matmul(p_attn,value), p_attn\n",
    "    \n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    Produce N identical layers\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    multihead attention, h is number of heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, h, keep_prob=0.1):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert d_model%h == 0\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.keep_prob = keep_prob\n",
    "        self.d_k = self.d_model//self.h\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.dropout = nn.Dropout(self.keep_prob)\n",
    "        self.linears = clones(nn.Linear(self.d_model,self.d_model), 4)\n",
    "        self.scaleddotattn = ScaledDotProductAttention(self.keep_prob)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            # same mask applied to all h heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        #nbatches = query.size(0)\n",
    "        #seq_len = query.size(1)\n",
    "        seq_len = inputs.size(1)\n",
    "        \n",
    "        # first, do all the linear projections in batch from d_model to h*d_k\n",
    "        query,key,value = [l(x).view(-1,seq_len,self.h,self.d_k).transpose(1,2) for l,x in zip(self.linears,(inputs, inputs, inputs))]\n",
    "        # second, apply attention on all the projected vectors in batch\n",
    "        x,self.attn = self.scaleddotattn(query,key,value,mask)\n",
    "        # third, concat using a view and apply a final linear\n",
    "        x = x.transpose(1,2).contiguous().view(-1,seq_len,self.h*self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    There is implementation in nn.LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.features = features\n",
    "        self.eps = eps\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.a_2 = nn.Parameter(torch.ones(self.features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(self.features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x-mean)/(std + self.eps)+self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9oM9mlqWEeX"
   },
   "outputs": [],
   "source": [
    "ps_proc = TextPhraseSimProcessor(labels=None, feature_columns=['sent', 'term'], label_columns='polarity')\n",
    "\n",
    "train_examples = ps_proc.get_train_examples(df=laptop_train_df, size=1000, labels_available=True)\n",
    "dev_examples = ps_proc.get_dev_examples(df=laptop_train_df, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "XytAK7HtXaKZ",
    "outputId": "458ad5db-520f-4560-eb03-edb2111952d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/hqian/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36872
    },
    "colab_type": "code",
    "id": "Z2yEAY81WEbK",
    "outputId": "02d41054-73e0-4fd9-87ab-2be6fadc7376"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 1000\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1351\n",
      "INFO:root:tokens: [CLS] the machine has a blur ##ay player the book has no mention of it or how to connect it to your hd ##tv . [SEP] blur ##ay player [SEP]\n",
      "INFO:root:input_ids: 101 1996 3698 2038 1037 14819 4710 2447 1996 2338 2038 2053 5254 1997 2009 2030 2129 2000 7532 2009 2000 2115 10751 9189 1012 102 14819 4710 2447 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 1)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 217\n",
      "INFO:root:tokens: [CLS] the board has a bad connector r with the power supply and shortly after warren ##ty ex ##pire ##s the power supply will start having issues . [SEP] connector [SEP]\n",
      "INFO:root:input_ids: 101 1996 2604 2038 1037 2919 19400 1054 2007 1996 2373 4425 1998 3859 2044 6031 3723 4654 20781 2015 1996 2373 4425 2097 2707 2383 3314 1012 102 19400 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: -1 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1725\n",
      "INFO:root:tokens: [CLS] many kinds of software that is necessary to the working person is not available and can not be downloaded . [SEP] software [SEP]\n",
      "INFO:root:input_ids: 101 2116 7957 1997 4007 2008 2003 4072 2000 1996 2551 2711 2003 2025 2800 1998 2064 2025 2022 22817 1012 102 4007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: -1 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1132\n",
      "INFO:root:tokens: [CLS] the graphics are great . [SEP] graphics [SEP]\n",
      "INFO:root:input_ids: 101 1996 8389 2024 2307 1012 102 8389 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 1 (id = 2)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 963\n",
      "INFO:root:tokens: [CLS] i have found it very easy to use , very im ##form ##ative , wonder alert ##s and tutor ##ials making it very easy for someone like me who is not exactly technological ##ly advanced to learn to use the various features and programs . [SEP] features [SEP]\n",
      "INFO:root:input_ids: 101 1045 2031 2179 2009 2200 3733 2000 2224 1010 2200 10047 14192 8082 1010 4687 9499 2015 1998 14924 26340 2437 2009 2200 3733 2005 2619 2066 2033 2040 2003 2025 3599 10660 2135 3935 2000 4553 2000 2224 1996 2536 2838 1998 3454 1012 102 2838 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 1 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing example 0 of 1000\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 442\n",
      "INFO:root:tokens: [CLS] i connect a lac ##ie 2 ##bi ##g external drive via the fire ##wire 800 interface , which is useful for time machine . [SEP] fire ##wire 800 interface [SEP]\n",
      "INFO:root:input_ids: 101 1045 7532 1037 18749 2666 1016 5638 2290 6327 3298 3081 1996 2543 20357 5385 8278 1010 2029 2003 6179 2005 2051 3698 1012 102 2543 20357 5385 8278 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 1 (id = 2)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 990\n",
      "INFO:root:tokens: [CLS] the only thing i wish is the 15 inch mac ##book pro has much better speakers on the side of the keyboard . [SEP] keyboard [SEP]\n",
      "INFO:root:input_ids: 101 1996 2069 2518 1045 4299 2003 1996 2321 4960 6097 8654 4013 2038 2172 2488 7492 2006 1996 2217 1997 1996 9019 1012 102 9019 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: 0 (id = 1)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 1863\n",
      "INFO:root:tokens: [CLS] my power supply cord developed exposed wires within the first year of ownership , so it was covered by the apple ##care warrant ##y plan . [SEP] power supply cord [SEP]\n",
      "INFO:root:input_ids: 101 2026 2373 4425 11601 2764 6086 14666 2306 1996 2034 2095 1997 6095 1010 2061 2009 2001 3139 2011 1996 6207 16302 10943 2100 2933 1012 102 2373 4425 11601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: -1 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 698\n",
      "INFO:root:tokens: [CLS] it has bei ##n into the shop to get a new hard ##ri ##ve 2 times and to fix the touch control buttons on the keyboard ! [SEP] keyboard [SEP]\n",
      "INFO:root:input_ids: 101 2009 2038 21388 2078 2046 1996 4497 2000 2131 1037 2047 2524 3089 3726 1016 2335 1998 2000 8081 1996 3543 2491 11287 2006 1996 9019 999 102 9019 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: -1 (id = 0)\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:guid: 861\n",
      "INFO:root:tokens: [CLS] in may i started having problems with the usb ports not working . [SEP] usb ports [SEP]\n",
      "INFO:root:input_ids: 101 1999 2089 1045 2318 2383 3471 2007 1996 18833 8831 2025 2551 1012 102 18833 8831 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:label: -1 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_data = get_batch(train_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)\n",
    "val_data = get_batch(dev_examples, label_map, max_seq_len, tokenizer, output_mode=\"classification\", label_available=True, batch_size=32, num_workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGH3yYVuWEYj"
   },
   "outputs": [],
   "source": [
    "params = {'n_labels':3, 'label_map':label_map, 'batch_size':16, 'n_epochs':10, 'seq_len':max_seq_len, 'n_workers':-1, 'lr':0.0001, 'keep_prob':0.1, 'heads':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1bAXX5M08Hc"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5867
    },
    "colab_type": "code",
    "id": "3CsI9SgCWEVw",
    "outputId": "44673cbf-2abe-45dd-e307-d6e24e6608c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/hqian/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/hqian/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/l_/cs7qvzls637f1fpbpkkny2_c0000gn/T/tmp1pba4kx1\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForAspect not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'multiheads.linears.0.weight', 'multiheads.linears.0.bias', 'multiheads.linears.1.weight', 'multiheads.linears.1.bias', 'multiheads.linears.2.weight', 'multiheads.linears.2.bias', 'multiheads.linears.3.weight', 'multiheads.linears.3.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForAspect: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForAspect(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (multiheads): MultiHeadAttention(\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (scaleddotattn): ScaledDotProductAttention(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_aspect_model = BertForAspect.from_pretrained('bert-base-uncased', params)\n",
    "\n",
    "bert_aspect_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLz9w4L1WESd"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, bert_aspect_model.parameters()), lr=params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9F_8wvPVyYR9"
   },
   "outputs": [],
   "source": [
    "bert_aspect_trainer = Trainer(device, batch_size=params['batch_size'], n_epochs=params['n_epochs'], min_clip_val=-1.0, max_clip_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "9Hu2IbezyX2I",
    "outputId": "08b8a6f2-7583-4519-9220-7ee369ce7312"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[AINFO:root:Epoch 1/10\n",
      "\n",
      "\n",
      "\n",
      "train_data_iter:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[AINFO:root:bert for aspect torch.Size([32, 474, 768])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-29ddad4b616c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_aspect_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_aspect_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-d245f6bbbf6f>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self, model, train_data, dev_data, optimizer, metrics, loss_fn, model_dir)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-d245f6bbbf6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_data, optimizer, sum_writer, epoch, metrics, loss_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# compute model output and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#print('train loss shape: ', loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hqian/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-18b7b693a7f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiheads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multihead pooled_output shape {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hqian/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "bert_aspect_trainer.train_and_evaluate(bert_aspect_model, train_data, val_data, optimizer, metrics, loss_fn=None, model_dir='./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8YIHAE1yXy7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaHvjOE1eRv4"
   },
   "source": [
    "# Bert For QA Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdodf9DjeVkH"
   },
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for Question Answering (span extraction).\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the sequence output that computes start_logits and end_logits\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "\n",
    "    Outputs:\n",
    "        if `start_positions` and `end_positions` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
    "        if `start_positions` or `end_positions` is `None`:\n",
    "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
    "            position tokens of shape [batch_size, sequence_length].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            return total_loss\n",
    "        else:\n",
    "            return start_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7gXADTeeWBq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGI5s_44eWEB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JutHsk7neWGb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjX8LuExeWI1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVhdI4L-eWLI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HWRsudMeWNm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqxBrSieeWPs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUVuYVhfGJOO"
   },
   "source": [
    "# Bert Multiple choice Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puB5brxdeWSf"
   },
   "outputs": [],
   "source": [
    "class BertForMultipleChoice(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for multiple choice tasks.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_choices`: the number of classes for the classifier. Default = 2.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n",
    "            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_choices].\n",
    "\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n",
    "    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n",
    "    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    num_choices = 2\n",
    "\n",
    "    model = BertForMultipleChoice(config, num_choices)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, params):\n",
    "        super(BertForMultipleChoice, self).__init__(config)\n",
    "        self.n_choices = params['n_choices']\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1) # 1 or n_labels\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_ids: (batch, seq_len), word index of text, start with [CLS] and end with [SEP] token ids\n",
    "          token_type_ids: (batch, seq_len), values from [0,1], indicates whether it's from sentence A(0) or B(1)\n",
    "          attention_mask: (batch, seq_len), mask for input text, values from [0,1], 1 means word is padded\n",
    "          labels: (batch), y \n",
    "        \"\"\"\n",
    "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        logging.info('flat_input_ids shape {}, flat_token_type_ids shape {}, flat_attention_mask shape {}'.format(flat_input_ids.shape, flat_token_type_ids.shape, flat_attention_mask.shape))\n",
    "        \n",
    "        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n",
    "        logging.info('pooled_output shape {}'.format(pooled_output.shape))\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # final prediction layer\n",
    "        logits = self.classifier(pooled_output)\n",
    "        self.logits = logits.view(-1, self.num_choices)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = CrossEntropyLoss()\n",
    "            self.loss = loss_fn(self.logits, labels)\n",
    "            return self.loss\n",
    "        else:\n",
    "            return self.logits\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbWKL_eweWWl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCaU6ljEJWqY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PbscZXmJWuf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frRao6QpJW3R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "obVAbeD8d3o6",
    "wki92f4br7Nv"
   ],
   "name": "bert_develop.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
